%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
 \ifdefined\DeclareUnicodeCharacterAsOptional
  \DeclareUnicodeCharacter{"00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{"2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{"2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{"2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{"251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{"2572}{\textbackslash}
 \else
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{2572}{\textbackslash}
 \fi
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage[dontkeepoldnames]{sphinx}

\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Quick Start}}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{0}



\title{InferPy Documentation}
\date{Oct 09, 2018}
\release{1.0}
\author{Rafael Cabañas}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex

\begin{document}

\maketitle
\sphinxtableofcontents
\phantomsection\label{\detokenize{index::doc}}


\noindent{\hspace*{\fill}\sphinxincludegraphics[scale=0.9]{{logo}.png}\hspace*{\fill}}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}

InferPy is a high-level API for probabilistic modeling written in Python and
capable of running on top of Tensorflow. InferPy’s API is
strongly inspired by Keras and it has a focus on enabling flexible data processing,
easy-to-code probablistic modeling, scalable inference and robust model validation.

Use InferPy if you need a probabilistic programming language that:
\begin{itemize}
\item {} 
Allows easy and fast prototyping of hierarchical probabilistic models with a simple and user friendly API inspired by Keras.

\item {} 
Automatically creates computational efficient batched models without the need to deal with complex tensor operations.

\item {} 
Run seamlessly on CPU and GPU by relying on Tensorflow, without having to learn how to use Tensorflow.

\end{itemize}


\chapter{Getting Started:}
\label{\detokenize{notes/getting30s:getting-started}}\label{\detokenize{notes/getting30s::doc}}\label{\detokenize{notes/getting30s:inferpy-probabilistic-modeling-with-tensorflow-made-easy}}

\section{Installation}
\label{\detokenize{notes/getting30s:installation}}
Install InferPy from PyPI:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pip install inferpy
\end{sphinxVerbatim}


\section{30 seconds to InferPy}
\label{\detokenize{notes/getting30s:seconds-to-inferpy}}
The core data structures of InferPy is a \sphinxstylestrong{probabilistic model},
defined as a set of \sphinxstylestrong{random variables} with a conditional dependency
structure. A \sphinxstylestrong{random varible} is an object
parameterized by a set of Numpy’s arrays.

Let’s look at a simple (Bayesian) \sphinxstylestrong{probabilistic component analysis} model. Graphically the model can
be defined as follows,

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.35]{{LinearFactor}.png}
\caption{Bayesian PCA}\label{\detokenize{notes/getting30s:id1}}\end{figure}

We start defining the \sphinxstylestrong{prior} of the global parameters,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{inferpy} \PYG{k+kn}{as} \PYG{n+nn}{inf}
\PYG{k+kn}{from} \PYG{n+nn}{inferpy.models} \PYG{k+kn}{import} \PYG{n}{Normal}

\PYG{c+c1}{\PYGZsh{} K defines the number of components.}
\PYG{n}{K}\PYG{o}{=}\PYG{l+m+mi}{10}

\PYG{c+c1}{\PYGZsh{} d defines the number of dimensions}
\PYG{n}{d}\PYG{o}{=}\PYG{l+m+mi}{20}

\PYG{c+c1}{\PYGZsh{}Prior for the principal components}
\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{n}{K}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{w} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim} \PYG{o}{=} \PYG{n}{d}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} x.shape = [K,d]}

\end{sphinxVerbatim}

InferPy supports the definition of \sphinxstylestrong{plateau notation} by using the
construct \sphinxcode{with inf.replicate(size = K)}, which replicates K times the
random variables enclosed within this anotator. Every replicated
variable is assumed to be \sphinxstylestrong{independent}.

This \sphinxcode{with inf.replicate(size = N)} construct is also useful when
defining the model for the data:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Number of observations}
\PYG{n}{N} \PYG{o}{=} \PYG{l+m+mi}{1000}

\PYG{c+c1}{\PYGZsh{} define the generative model}
\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{N}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{z} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{K}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} z.shape = [N,K]}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{z}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} x.shape = [N,d]}

\end{sphinxVerbatim}

As commented above, the variables are surrounded by a
\sphinxcode{with} statement to inidicate that the defined random variables will
be reapeatedly used in each data sample. In this case, every replicated
variable is conditionally idependent given the variable \(\mathbf{w}\)
defined above.

Once the random variables of the model are defined, the probablitic
model itself can be created and compiled. The probabilistic model
defines a joint probability distribuiton over all these random
variables.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{inferpy} \PYG{k+kn}{import} \PYG{n}{ProbModel}

\PYG{c+c1}{\PYGZsh{} Define the model}
\PYG{n}{pca} \PYG{o}{=} \PYG{n}{ProbModel}\PYG{p}{(}\PYG{n}{varlist} \PYG{o}{=} \PYG{p}{[}\PYG{n}{w}\PYG{p}{,}\PYG{n}{z}\PYG{p}{,}\PYG{n}{x}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Compile the model}
\PYG{n}{pca}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{infMethod} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{KLqp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

During the model compilation we specify different inference methods that
will be used to learn the model.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{inferpy} \PYG{k+kn}{import} \PYG{n}{ProbModel}

\PYG{c+c1}{\PYGZsh{} Define the model}
\PYG{n}{pca} \PYG{o}{=} \PYG{n}{ProbModel}\PYG{p}{(}\PYG{n}{varlist} \PYG{o}{=} \PYG{p}{[}\PYG{n}{w}\PYG{p}{,}\PYG{n}{z}\PYG{p}{,}\PYG{n}{x}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Compile the model}
\PYG{n}{pca}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{infMethod} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variational}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

The inference method can be further configure. But, as in Keras, a core
principle is to try make things reasonbly simple, while allowing the
user the full control if needed.

Every random variable object is equipped with methods such as
\sphinxcode{log\_prob()} and \sphinxcode{sample()}. Similarly, a probabilistic model is also
equipped with the same methods. Then, we can sample data from the model
anbd compute the log-likelihood of a data set:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Sample data from the model}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pca}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{l+m+mi}{100}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Compute the log\PYGZhy{}likelihood of a data set}
\PYG{n}{log\PYGZus{}like} \PYG{o}{=} \PYG{n}{pca}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}
\end{sphinxVerbatim}

Of course, you can fit your model with a given data set:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} compile and fit the model with training data}
\PYG{n}{pca}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{pca}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}extract the hidden representation from a set of observations}
\PYG{n}{hidden\PYGZus{}encoding} \PYG{o}{=} \PYG{n}{pca}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}
\end{sphinxVerbatim}


\chapter{Guiding Principles}
\label{\detokenize{notes/gettingGuiding::doc}}\label{\detokenize{notes/gettingGuiding:guiding-principles}}

\section{Features}
\label{\detokenize{notes/gettingGuiding:features}}
The main features of InferPy are listed below.
\begin{itemize}
\item {} 
The models that can be defined in Inferpy are those that can be defined using Edward, whose probability distribuions
are mainly inherited from TensorFlow Distribuitons package.

\item {} 
Edward’s drawback is that for the model definition, the user has to manage complex multidimensional arrays called
tensors. By contrast, in InferPy all the parameters in a model can be defined using the standard Python types
(compatibility with Numpy is available as well).

\item {} 
InferPy directly relies on top of Edward’s inference engine and
includes all the inference algorithms included in this package. As
Edward’s inference engine relies on TensorFlow computing engine,
InferPy also relies on it too.

\item {} 
InferPy seamlessly process data contained in a numpy array, Tensorflow’s
tensor, Tensorflow’s Dataset (tf.Data API), Pandas’ DataFrame or Apache Spark’s
DataFrame.

\item {} 
InferPy also includes novel distributed statistical inference
algorithms by combining Tensorflow computing
engines.

\end{itemize}


\section{Architecture}
\label{\detokenize{notes/gettingGuiding:architecture}}
Given the previous considerations, we might summarize the InferPy architecture as follows.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.35]{{inferpy_architecture_simple}.png}
\end{figure}

Note that InferPy can be seen as an upper layer for working with probabilistic distributions defined
over tensors. Most of the interaction is done with Edward:  the definitions of the distributions, the
inference. However, InferPy also interacts directly with Tensorflow in some operations that are hidden to
the user, e.g. the manipulation of the tensors representing the parameters of the distributions.

An additional advantage of using Edward and Tensorflow as inference engine, is that all the paralelisation details
are hidden to the user. Moreover, the same code will run either in CPUs or GPUs.

For some less important task, InferPy might also interact with other third-party software. For example, reading data is
done with Pandas or the visualization tasks are leveraged to MatPlotLib.


\chapter{Requirements}
\label{\detokenize{notes/requirements:requirements}}\label{\detokenize{notes/requirements::doc}}

\section{Python}
\label{\detokenize{notes/requirements:python}}
Currently, InferPy requires Python 2.7 or 3.x. For checking your default Python version, type:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} python \PYGZhy{}\PYGZhy{}version
\end{sphinxVerbatim}

Travis tests are performed on versions 2.7, 3.5 and 3.6. Go to \sphinxurl{https://www.python.org/}
for specific instructions for installing the Python interpreter in your system.


\section{Edward}
\label{\detokenize{notes/requirements:edward}}
InferPy requires exactly the version 1.3.5 of \sphinxhref{http://edwardlib.org}{Edward}. You may check the installed
package version as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pip freeze \textbar{} grep edward
\end{sphinxVerbatim}


\section{Tensorflow}
\label{\detokenize{notes/requirements:tensorflow}}
\sphinxhref{http://www.tensorflow.org/}{Tensorflow}: from version 1.5 up to 1.7 (both included). To check the installed tensorflow version, type:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pip freeze \textbar{} grep tensorflow
\end{sphinxVerbatim}


\section{Numpy}
\label{\detokenize{notes/requirements:numpy}}
\sphinxhref{http://www.numpy.org/}{Numpy} 1.14 or higher is required. To check the version of this package, type:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pip freeze \textbar{} grep numpy
\end{sphinxVerbatim}


\section{Pandas}
\label{\detokenize{notes/requirements:pandas}}
\sphinxhref{https://pandas.pydata.org}{Pandas} 0.15.0 or higher is required. The installed version of this package can be checked as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pip freeze \textbar{} grep pandas
\end{sphinxVerbatim}


\chapter{Guide to Building Probabilistic Models}
\label{\detokenize{notes/guidemodels:guide-to-building-probabilistic-models}}\label{\detokenize{notes/guidemodels::doc}}

\section{Getting Started with Probabilistic Models}
\label{\detokenize{notes/guidemodels:getting-started-with-probabilistic-models}}
InferPy focuses on \sphinxstyleemphasis{hirearchical probabilistic models} structured
in two different layers:
\begin{itemize}
\item {} 
A \sphinxstylestrong{prior model} defining a joint distribution \(p(\mathbf{w})\)
over the global parameters of the model. \(\mathbf{w}\) can be a single random
variable or a bunch of random variables with any given dependency structure.

\item {} 
A \sphinxstylestrong{data or observation model} defining a joint conditional
distribution \(p(\mathbf{x},\mathbf{z}|\mathbf{w})\) over the observed quantities
\(\mathbf{x}\) and the the local hidden variables \(\mathbf{z}\) governing the
observation \(\mathbf{x}\). This data model is specified in a
single-sample basis. There are many models of interest without local
hidden variables, in that case, we simply specify the conditional
\(p(\mathbf{x}|\mathbf{w})\). Similarly, either \(\mathbf{x}\) or
\(\mathbf{z}\) can be a single random variable or a bunch of random variables
with any given dependency structure.

\end{itemize}

For example, a Bayesian PCA model has the following graphical structure,

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.35]{{LinearFactor}.png}
\caption{Bayesian PCA}
\begin{sphinxlegend}\begin{quote}

The \sphinxstylestrong{prior model} are the variables \(w_k\). The \sphinxstylestrong{data model} is the part of the model surrounded by the box indexed by \sphinxstylestrong{N}.
\end{quote}
\end{sphinxlegend}
\label{\detokenize{notes/guidemodels:id1}}\end{figure}

And this is how this Bayesian PCA model is denfined in InferPy:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{edward} \PYG{k+kn}{as} \PYG{n+nn}{ed}
\PYG{k+kn}{import} \PYG{n+nn}{inferpy} \PYG{k+kn}{as} \PYG{n+nn}{inf}
\PYG{k+kn}{from} \PYG{n+nn}{inferpy.models} \PYG{k+kn}{import} \PYG{n}{Normal}

\PYG{n}{K}\PYG{p}{,} \PYG{n}{d}\PYG{p}{,} \PYG{n}{N} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{200}

\PYG{c+c1}{\PYGZsh{} model definition}
\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{}define the weights}
    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{K}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{w} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} define the generative model}
    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{N}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{z} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{K}\PYG{p}{)}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{z}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}

\PYG{n}{m}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{p}{)}


\end{sphinxVerbatim}

The \sphinxcode{with inf.replicate(size = N)} sintaxis is used to replicate the
random variables contained within this construct. It follows from the
so-called \sphinxstyleemphasis{plateau notation} to define the data generation part of a
probabilistic model. Every replicated variable is \sphinxstylestrong{conditionally
idependent} given the previous random variables (if any) defined
outside the \sphinxstylestrong{with} statement.


\section{Random Variables}
\label{\detokenize{notes/guidemodels:random-variables}}
Following Edward’s approach, a random variable \(x\) is an object
parametrized by a tensor \(\theta\) (i.e. a TensorFlow’s tensor or
numpy’s ndarray). The number of random variables in one object is
determined by the dimensions of its parameters (like in Edward) or by
the ‘dim’ argument (inspired by PyMC3 and Keras):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{inferpy} \PYG{k+kn}{as} \PYG{n+nn}{inf}
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow} \PYG{k+kn}{as} \PYG{n+nn}{tf}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k+kn}{as} \PYG{n+nn}{np}


\PYG{c+c1}{\PYGZsh{} different ways of declaring 1 batch of 5 Normal distributions}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}         \PYG{c+c1}{\PYGZsh{} x.shape = [5]}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} x.shape = [5]}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} x.shape = [5]}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}       \PYG{c+c1}{\PYGZsh{} x.shape = [5]}
\end{sphinxVerbatim}

The \sphinxcode{with inf.replicate(size = N)} sintaxis can also be used to define
multi-dimensional objects:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}       \PYG{c+c1}{\PYGZsh{} x.shape = [10,5]}
\end{sphinxVerbatim}

Following Edward’s approach, the multivariate dimension is the innermost (right-most)
dimension of the parameters.

Note that indexing is supported:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y} \PYG{o}{=} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{]}                                              \PYG{c+c1}{\PYGZsh{} y.shape = [1]}

\PYG{n}{y2} \PYG{o}{=} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{7}\PYG{p}{]}                                               \PYG{c+c1}{\PYGZsh{} y2.shape = [5]}

\PYG{n}{y3} \PYG{o}{=} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}                                             \PYG{c+c1}{\PYGZsh{} y2.shape = [5]}

\PYG{n}{y4} \PYG{o}{=} \PYG{n}{x}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{]}                                             \PYG{c+c1}{\PYGZsh{} y4.shape = [10]}
\end{sphinxVerbatim}

Moreover, we may use indexation for defining new variables whose indexes may be other (discrete) variables:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{z} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{logits} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{yz} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{z}\PYG{p}{]}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}            \PYG{c+c1}{\PYGZsh{} yz.shape = [1]}
\end{sphinxVerbatim}

Any random variable in InferPy contain the following (optional) input parameters
in the constructor:
\begin{itemize}
\item {} 
\sphinxcode{validate\_args} : Python boolean indicating that possibly expensive checks with the input parameters are enabled.
By default, it is set to \sphinxcode{False}.

\item {} 
\sphinxcode{allow\_nan\_stats} : When \sphinxcode{True}, the value “NaN” is used to indicate the result is undefined. Otherwise an exception is raised.
Its default value is \sphinxcode{True}.

\item {} 
\sphinxcode{name}: Python string with the name of the underlying Tensor object.

\item {} 
\sphinxcode{observed}: Python boolean which is used to indicate whether a variable is observable or not . The default value is \sphinxcode{False}

\item {} 
\sphinxcode{dim}: dimension of the variable. The default value is \sphinxcode{None}

\end{itemize}

Inferpy supports a wide range of probability distributions. Details of the specific arguments
for each supported distributions are specified in the following sections.
{}`


\section{Probabilistic Models}
\label{\detokenize{notes/guidemodels:probabilistic-models}}
A \sphinxstylestrong{probabilistic model} defines a joint distribution over observable
and non-observable variables, \(p(\mathbf{w}, \mathbf{z}, \mathbf{x})\) for the
running example. The variables in the model are the ones defined using the
\sphinxcode{with inf.ProbModel() as pca:} construct. Alternatively, we can also use a builder,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{m} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{n}{varlist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{w}\PYG{p}{,}\PYG{n}{z}\PYG{p}{,}\PYG{n}{x}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

The model must be \sphinxstylestrong{compiled} before it can be used.

Like any random variable object, a probabilistic model is equipped with
methods such as  \sphinxcode{sample()}, \sphinxcode{log\_prob()} and  \sphinxcode{sum\_log\_prob()}. Then, we can sample data
from the model and compute the log-likelihood of a data set:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data} \PYG{o}{=} \PYG{n}{m}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)}
\PYG{n}{log\PYGZus{}like} \PYG{o}{=} \PYG{n}{m}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}
\PYG{n}{sum\PYGZus{}log\PYGZus{}like} \PYG{o}{=} \PYG{n}{m}\PYG{o}{.}\PYG{n}{sum\PYGZus{}log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}
\end{sphinxVerbatim}

Random variables can be involved in expressive deterministic operations. Dependecies
between variables are modelled by setting a given variable as a parameter of another variable. For example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m}\PYG{p}{:}
    \PYG{n}{theta} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Beta}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{)}
    \PYG{n}{z} \PYG{o}{=}  \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{p}{[}\PYG{n}{theta}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{theta}\PYG{p}{]}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{z}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}


\PYG{n}{m}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

Moreover, we might consider using the function \sphinxcode{inferpy.case} as the parameter of other random variables:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Categorical variable depending on another categorical variable}

\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m2}\PYG{p}{:}
    \PYG{n}{y} \PYG{o}{=}  \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,}\PYG{l+m+mf}{0.6}\PYG{p}{]}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{case}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{y}\PYG{o}{.}\PYG{n}{equal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{]}\PYG{p}{,}
                                               \PYG{n}{y}\PYG{o}{.}\PYG{n}{equal}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]} \PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{x}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{m2}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} Categorical variable depending on a Normal distributed variable}

\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m3}\PYG{p}{:}
    \PYG{n}{a} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{a}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{b} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{case}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{a}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{]}\PYG{p}{,}
                                               \PYG{n}{a}\PYG{o}{\PYGZlt{}}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{b}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{m3}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} Normal distributed variable depending on a Categorical variable}

\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m4}\PYG{p}{:}
    \PYG{n}{d} \PYG{o}{=}  \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,}\PYG{l+m+mf}{0.6}\PYG{p}{]}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{d}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{c} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{case}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{d}\PYG{o}{.}\PYG{n}{equal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:} \PYG{l+m+mf}{0.}\PYG{p}{,}
                                        \PYG{n}{d}\PYG{o}{.}\PYG{n}{equal}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:} \PYG{l+m+mf}{100.}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{c}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{m4}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

Note that we might use the case function inside the replicate construct. The result will be
a multi-batch random variable having the same distribution for each batch. When obtaining a sample from
the model, each sample of a given batch in x is independent of the rest.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m}\PYG{p}{:}
    \PYG{n}{y} \PYG{o}{=}  \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,}\PYG{l+m+mf}{0.6}\PYG{p}{]}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{case}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{y}\PYG{o}{.}\PYG{n}{equal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{,}
                                                    \PYG{n}{y}\PYG{o}{.}\PYG{n}{equal}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]} \PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{x}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

We can also use the functions \sphinxcode{inferpy.case\_states} or \sphinxcode{inferpy.gather} for defining
the same model.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m}\PYG{p}{:}
    \PYG{n}{y} \PYG{o}{=}  \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,}\PYG{l+m+mf}{0.6}\PYG{p}{]}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{case\PYGZus{}states}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{]}\PYG{p}{,}
                                                         \PYG{l+m+mi}{1}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]} \PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{x}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}


\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m}\PYG{p}{:}
    \PYG{n}{y} \PYG{o}{=}  \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,}\PYG{l+m+mf}{0.6}\PYG{p}{]}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{gather}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{x}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{m}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

We can use the function \sphinxcode{inferpy.case\_states} with a list of variables (or multidimensional variables):

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\PYG{n}{y} \PYG{o}{=}  \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{p} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{case\PYGZus{}states}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{p}{\PYGZob{}}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
                        \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{]}\PYG{p}{\PYGZcb{}} \PYG{p}{)}

\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{n}{p}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{x}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}}

\PYG{n}{y} \PYG{o}{=}  \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{z} \PYG{o}{=}  \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{z}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}


\PYG{n}{p} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{case\PYGZus{}states}\PYG{p}{(}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,}\PYG{n}{z}\PYG{p}{)}\PYG{p}{,} \PYG{p}{\PYGZob{}}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
                            \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{]}\PYG{p}{\PYGZcb{}} \PYG{p}{)}

\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{n}{p}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{x}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}}

\PYG{n}{p} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{case\PYGZus{}states}\PYG{p}{(}\PYG{p}{[}\PYG{n}{y}\PYG{p}{,}\PYG{n}{z}\PYG{p}{]}\PYG{p}{,} \PYG{p}{\PYGZob{}}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
                            \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{]}\PYG{p}{\PYGZcb{}} \PYG{p}{)}

\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{n}{p}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{x}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\end{sphinxVerbatim}


\section{Supported Probability Distributions}
\label{\detokenize{notes/guidemodels:supported-probability-distributions}}
Supported probability distributions are located in the package \sphinxcode{inferpy.models}. All of them
have \sphinxcode{inferpy.models.RandomVariable} as superclass. A list with all the supported distributions can be obtained as
as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{ALLOWED\PYGZus{}VARS}
\PYG{g+go}{[\PYGZsq{}Bernoulli\PYGZsq{}, \PYGZsq{}Beta\PYGZsq{}, \PYGZsq{}Categorical\PYGZsq{}, \PYGZsq{}Deterministic\PYGZsq{}, \PYGZsq{}Dirichlet\PYGZsq{}, \PYGZsq{}Exponential\PYGZsq{}, \PYGZsq{}Gamma\PYGZsq{}, \PYGZsq{}InverseGamma\PYGZsq{}, \PYGZsq{}Laplace\PYGZsq{}, \PYGZsq{}Multinomial\PYGZsq{}, \PYGZsq{}Normal\PYGZsq{}, \PYGZsq{}Poisson\PYGZsq{}, \PYGZsq{}Uniform\PYGZsq{}]}
\end{sphinxVerbatim}


\subsection{Bernoulli}
\label{\detokenize{notes/guidemodels:bernoulli}}
Binary distribution which takes the value 1 with probability \(p\) and the value with \(1-p\). Its probability mass
function is
\begin{equation*}
\begin{split}p(x;p) =\left\{\begin{array}{cc} p & \mathrm{if\ } x=1 \\
 1-p & \mathrm{if\ } x=0 \\ \end{array} \right.\end{split}
\end{equation*}
An example of definition in InferPy of a random variable following a Bernoulli distribution is shown below. Note that the
input parameter \sphinxcode{probs} corresponds to \(p\) in the previous equation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Bernoulli}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} or}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Bernoulli}\PYG{p}{(}\PYG{n}{logits}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

This distribution can be initialized by indicating the logit function of the probability, i.e., \(logit(p) = log(\frac{p}{1-p})\).


\subsection{Beta}
\label{\detokenize{notes/guidemodels:beta}}
Continuous distribution defined in the interval \([0,1]\) and parametrized by two positive shape parameters,
denoted \(\alpha\) and \(\beta\).
\begin{equation*}
\begin{split}p(x;\alpha,\beta)=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}\end{split}
\end{equation*}
where \sphinxtitleref{B} is the beta function
\begin{equation*}
\begin{split}B(\alpha,\beta)=\int_{0}^{1}t^{\alpha-1}(1-t)^{\beta-1}dt\end{split}
\end{equation*}
The definition of a random variable following a Beta distribution is done as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Beta}\PYG{p}{(}\PYG{n}{concentration0}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{n}{concentration1}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} or simply:}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Beta}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\end{sphinxVerbatim}

Note that the input parameters \sphinxcode{concentration0} and \sphinxcode{concentration1} correspond to the shape
parameters \(\alpha\) and \(\beta\) respectively.


\subsection{Categorical}
\label{\detokenize{notes/guidemodels:categorical}}
Discrete probability distribution that can take \(k\) possible states or categories. The probability
of each state is separately defined:
\begin{equation*}
\begin{split}p(x;\mathbf{p}) = p_i\end{split}
\end{equation*}
where \(\mathbf{p} = (p_1, p_2, \ldots, p_k)\) is a k-dimensional vector with the probability associated to each possible state.

The definition of a random variable following a Categorical distribution is done as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} or}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{logits}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Deterministic}
\label{\detokenize{notes/guidemodels:deterministic}}
The deterministic distribution is a probability distribution in a space (continuous or discrete) that always takes
the same value \(k_0\). Its probability density (or mass) function can be defined as follows.
\begin{equation*}
\begin{split}p(x;k_0) =\left\{\begin{array}{cc} 1 & \mathrm{if\ } x=k_0 \\
 0 & \mathrm{if\ } x \neq k_0 \\ \end{array} \right.\end{split}
\end{equation*}
The definition of a random variable following a Beta distribution is done as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Deterministic}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}
\end{sphinxVerbatim}

where the input parameter \sphinxcode{loc} corresponds to the value \(k_0\).


\subsection{Dirichlet}
\label{\detokenize{notes/guidemodels:dirichlet}}
Dirichlet distribution is a continuous multivariate probability distribution parmeterized by a vector of positive reals
\((\alpha_1,\alpha_2,\ldots,\alpha_k)\).
It is a multivariate generalization of the beta distribution. Dirichlet distributions are commonly used as prior
distributions in Bayesian statistics. The Dirichlet distribution of order \(k \geq 2\) has the following density function.
\begin{equation*}
\begin{split}p(x_1,x_2,\ldots x_k; \alpha_1,\alpha_2,\ldots,\alpha_k) = {\frac{\Gamma\left(\sum_i \alpha_i\right)}
{\prod_i \Gamma(\alpha_i)} \prod_{i=1}^k x_i^{\alpha_i-1}}{}\end{split}
\end{equation*}
The definition of a random variable following a Beta distribution is done as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Dirichlet}\PYG{p}{(}\PYG{n}{concentration}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} or simply:}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Dirichlet}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

where the input parameter \sphinxcode{concentration} is the vector  \((\alpha_1,\alpha_2,\ldots,\alpha_k)\).


\subsection{Exponential}
\label{\detokenize{notes/guidemodels:exponential}}
The exponential distribution (also known as negative exponential distribution) is defined over a continuous domain and
describes the time between events in a Poisson point process, i.e., a process in which events occur continuously
and independently at a constant average rate. Its probability density function is
\begin{equation*}
\begin{split}p(x;\lambda) =\left\{\begin{array}{cc} \lambda e^{-\lambda x} & \mathrm{if\ } x\geq 0 \\
 0 & \mathrm{if\ } x < k_0 \\ \end{array} \right.\end{split}
\end{equation*}
where \(\lambda>0\) is the rate or inverse scale.

The definition of a random variable following a exponential distribution is done as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Exponential}\PYG{p}{(}\PYG{n}{rate}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} or simply}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Exponential}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

where the input parameter \sphinxcode{rate} corresponds to the value \(\lambda\).


\subsection{Gamma}
\label{\detokenize{notes/guidemodels:gamma}}
The Gamma distribution is a continuous probability distribution parametrized by a concentration (or shape)
parameter \(\alpha>0\), and an inverse scale parameter \(\lambda>0\) called rate. Its density function is
defined as follows.
\begin{equation*}
\begin{split}p(x;\alpha, \beta) = \frac{\beta^\alpha x^{\alpha - 1} e^{\beta x}}{\Gamma(\alpha)}\end{split}
\end{equation*}
for \(x > 0\) and where \(\Gamma(\alpha)\) is the gamma function.

The definition of a random variable following a gamma distribution is done as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Gamma}\PYG{p}{(}\PYG{n}{concentration}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{rate}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

where the input parameters \sphinxcode{concentration} and \sphinxcode{rate} corespond to  \(\alpha\) and \(\beta\) respectively.


\subsection{Inverse-gamma}
\label{\detokenize{notes/guidemodels:inverse-gamma}}
The Inverse-gamma distribution is a continuous probability distribution which is the distribution of the reciprocal
of a variable distributed according to the gamma distribution. It is also parametrized by a concentration (or shape)
parameter \(\alpha>0\), and an inverse scale parameter \(\lambda>0\) called rate. Its density function is
defined as follows.
\begin{equation*}
\begin{split}p(x;\alpha, \beta) = \frac{\beta^\alpha x^{-\alpha - 1} e^{-\frac{\beta}{x}}}{\Gamma(\alpha)}\end{split}
\end{equation*}
for \(x > 0\) and where \(\Gamma(\alpha)\) is the gamma function.

The definition of a random variable following a inverse-gamma distribution is done as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{InverseGamma}\PYG{p}{(}\PYG{n}{concentration}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{rate}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

where the input parameters \sphinxcode{concentration} and \sphinxcode{rate} corespond to  \(\alpha\) and \(\beta\) respectively.


\subsection{Laplace}
\label{\detokenize{notes/guidemodels:laplace}}
The Laplace distribution is a continuous probability distribution with the following density function
\begin{equation*}
\begin{split}p(x;\mu,\sigma) = \frac{1}{2\sigma} exp \left( - \frac{|x - \mu |}{\sigma}\right)\end{split}
\end{equation*}
The definition of a random variable following a Beta distribution is done as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Laplace}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} or simply}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Laplace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

where the input parameter \sphinxcode{loc} and \sphinxcode{scale} correspond to \(\mu\) and \(\sigma\) respectively.


\subsection{Multinomial}
\label{\detokenize{notes/guidemodels:multinomial}}
The multinomial is a discrete distribution which models the probability of counts resulting from repeating \(n\)
times an experiment with \(k\) possible outcomes. Its probability mass function is defined below.
\begin{equation*}
\begin{split}p(x_1,x_2,\ldots x_k; \mathbf{p}) =  \frac{n!}{\prod_{i=1}^k x_i}\prod_{i=1}^k p_i^{x^i}\end{split}
\end{equation*}
where \(\mathbf{p}\) is a k-dimensional vector defined as \(\mathbf{p} = (p_1, p_2, \ldots, p_k)\) with the probability
associated to each possible outcome.

The definition of a random variable following a multinomial distribution is done as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Multinomial}\PYG{p}{(}\PYG{n}{total\PYGZus{}count}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{probs}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} or}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Multinomial}\PYG{p}{(}\PYG{n}{total\PYGZus{}count}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{logits}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Multivariate-Normal}
\label{\detokenize{notes/guidemodels:multivariate-normal}}
A multivariate-normal (or Gaussian) defines a set of  normal-distributed variables which are assumed
to be idependent. In other words, the covariance matrix is diagonal.

A single multivariate-normal distribution defined on \(\mathbb{R}^2\) can be defined as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{MultivariateNormalDiag}\PYG{p}{(}
    \PYG{n}{loc}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{scale\PYGZus{}diag}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mf}{2.}\PYG{p}{]}
\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Normal}
\label{\detokenize{notes/guidemodels:normal}}
The normal (or Gaussian) distribution is a continuous probability distribution with the following density function
\begin{equation*}
\begin{split}p(x;\mu,\sigma) = \frac{1}{2\sigma} exp \left( - \frac{|x - \mu |}{\sigma}\right)\end{split}
\end{equation*}
where \(\mu\)  is the mean or expectation of the distribution, \(\sigma\)  is the standard deviation, and \(\sigma^{2}\) is the variance.

A normal distribution can be defined as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} or}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

where the input parameter \sphinxcode{loc} and \sphinxcode{scale} correspond to \(\mu\) and \(\sigma\) respectively.


\subsection{Poisson}
\label{\detokenize{notes/guidemodels:poisson}}
The Poisson distribution is a discrete probability distribution for modeling the number of times an event occurs
in an interval of time or space. Its probability mass function is
\begin{equation*}
\begin{split}p(x;\lambda) = e^{- \lambda} \frac{\lambda^x}{x!}\end{split}
\end{equation*}
where \(\lambda\) is the rate or number of events per interval.

A Poisson distribution can be defined as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Poisson}\PYG{p}{(}\PYG{n}{rate}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} or}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Poisson}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Uniform}
\label{\detokenize{notes/guidemodels:uniform}}
The continuous uniform distribution or rectangular distribution assings the same probability to any \(x\)  in
the interval \([a,b]\).
\begin{equation*}
\begin{split}p(x;a,b) =\left\{\begin{array}{cc} \frac{1}{b-a} & \mathrm{if\ } x\in [a,b]\\
 0 & \mathrm{if\ } x\not\in [a,b] \\ \end{array} \right.\end{split}
\end{equation*}
A uniform distribution can be defined as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Uniform}\PYG{p}{(}\PYG{n}{low}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{high}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} or}

\PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Uniform}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

where the input parameters \sphinxcode{low} and \sphinxcode{high} correspond to the lower and upper bounds of the interval \([a,b]\).


\chapter{Guide to Approximate Inference}
\label{\detokenize{notes/guideinference::doc}}\label{\detokenize{notes/guideinference:guide-to-approximate-inference}}

\section{Getting Started with Approximate Inference}
\label{\detokenize{notes/guideinference:getting-started-with-approximate-inference}}
The API defines the set of algorithms and methods used to perform
inference in a probabilistic model \(p(x,z,\theta)\) (where
\(x\) are the observations, \(z\) the local hidden variibles,
and \(\theta\) the global parameters of the model). More precisely,
the inference problem reduces to compute the posterior probability over
the latent variables given a data sample
\(p(z,\theta | x_{train})\), because by looking at these
posteriors we can uncover the hidden structure in the data. For the
running example, the posterior over the local hidden variables \(p(w_n|x_{train})\)
tell us the latent vector representation of the sample \(x_n\), while the posterior
over the global variables \(p(\mu|x_{train})\) tells us which is the affine transformation
between the latent space and the observable space.

InferPy inherits Edward’s approach an consider approximate inference
solutions,
\begin{equation*}
\begin{split}q(z,\theta) \approx p(z,\theta | x_{train})\end{split}
\end{equation*}
in which the task is to approximate the posterior
\(p(z,\theta | x_{train})\) using a family of distributions,
\(q(z,\theta; \lambda)\), indexed by a parameter vector
\(\lambda\).

A probabilistic model in InferPy should be compiled before we can access
these posteriors,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{m}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{infMethod}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{KLqp}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{x\PYGZus{}train}\PYG{p}{)}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}
\end{sphinxVerbatim}

The compilation process allows to choose the inference algorithm through
the \sphinxcode{infMethod} argument. In the above example we use \sphinxcode{'Klqp'}.

Following InferPy guiding principles, users can further configure the
inference algorithm. First, they can define a model ‘Q’ for approximating the
posterior distribution,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{qw} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{Qmodel}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{w}\PYG{p}{)}
\PYG{n}{qz} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{Qmodel}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}

\PYG{n}{qmodel} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{Qmodel}\PYG{p}{(}\PYG{p}{[}\PYG{n}{qw}\PYG{p}{,} \PYG{n}{qz}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{m}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{infMethod}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{KLqp}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{Q}\PYG{o}{=}\PYG{n}{qmodel}\PYG{p}{)}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{x\PYGZus{}train}\PYG{p}{)}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}
\end{sphinxVerbatim}

In the ‘Q’ model we should include a q distribution for every non observed variable in
the ‘P’ model. Otherwise, an error will be raised during model compilation.

By default, the posterior \sphinxstylestrong{q} belongs to the same distribution family
than \sphinxstylestrong{p} , but in the above example we show how we can change that
(e.g. we set the posterior over \sphinxstylestrong{mu} to obtain a point mass estimate
instead of the Gaussian approximation used by default). We can also
configure how these \sphinxstylestrong{q’s} are initialized using any of the Keras’s
initializers.


\section{Compositional Inference}
\label{\detokenize{notes/guideinference:compositional-inference}}
\begin{sphinxadmonition}{note}{Note:}
not implemented yet
\end{sphinxadmonition}

InferPy directly builds on top of Edward’s compositionality idea to design complex
infererence algorithms.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pca} \PYG{o}{=} \PYG{n}{ProbModel}\PYG{p}{(}\PYG{n+nb}{vars} \PYG{o}{=} \PYG{p}{[}\PYG{n}{mu}\PYG{p}{,}\PYG{n}{w\PYGZus{}n}\PYG{p}{,}\PYG{n}{x\PYGZus{}n}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{q\PYGZus{}mu} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{inference}\PYG{o}{.}\PYG{n}{Q}\PYG{o}{.}\PYG{n}{PointMass}\PYG{p}{(}\PYG{n}{bind} \PYG{o}{=} \PYG{n}{mu}\PYG{p}{,} \PYG{n}{initializer}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{zeroes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{q\PYGZus{}w\PYGZus{}n} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{inference}\PYG{o}{.}\PYG{n}{Q}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{bind} \PYG{o}{=} \PYG{n}{w\PYGZus{}n}\PYG{p}{,} \PYG{n}{initializer}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{random\PYGZus{}unifrom}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{qlocal} \PYG{o}{=} \PYG{n}{QModel}\PYG{p}{(}\PYG{n+nb}{vars} \PYG{o}{=} \PYG{p}{[}\PYG{n}{q\PYGZus{}w\PYGZus{}n}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{qglobal} \PYG{o}{=} \PYG{n}{QModel}\PYG{p}{(}\PYG{n+nb}{vars} \PYG{o}{=} \PYG{p}{[}\PYG{n}{mu}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{infkl\PYGZus{}qp} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{inference}\PYG{o}{.}\PYG{n}{KLqp}\PYG{p}{(}\PYG{n}{Q} \PYG{o}{=} \PYG{n}{qlocal}\PYG{p}{,} \PYG{n}{optimizer} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sgd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{innerIter} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{infMAP} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{inference}\PYG{o}{.}\PYG{n}{MAP}\PYG{p}{(}\PYG{n}{Q} \PYG{o}{=} \PYG{n}{qglobal}\PYG{p}{,} \PYG{n}{optimizer} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sgd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{sgd} \PYG{o}{=} \PYG{n}{keras}\PYG{o}{.}\PYG{n}{optimizers}\PYG{o}{.}\PYG{n}{SGD}\PYG{p}{(}\PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{n}{momentum}\PYG{o}{=}\PYG{l+m+mf}{0.9}\PYG{p}{,} \PYG{n}{nesterov}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{infkl\PYGZus{}qp} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{inference}\PYG{o}{.}\PYG{n}{KLqp}\PYG{p}{(}\PYG{n}{Q} \PYG{o}{=} \PYG{n}{qmodel}\PYG{p}{,} \PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{sgd}\PYG{p}{,} \PYG{n}{loss}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ELBO}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{probmodel}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{infMethod} \PYG{o}{=} \PYG{p}{[}\PYG{n}{infkl\PYGZus{}qp}\PYG{p}{,}\PYG{n}{infMAP}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{pca}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{x\PYGZus{}train}\PYG{p}{)}
\PYG{n}{posterior\PYGZus{}mu} \PYG{o}{=} \PYG{n}{pca}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{n}{mu}\PYG{p}{)}
\end{sphinxVerbatim}

With the above sintaxis, we perform a variational EM algorithm, where
the E step is repeated 10 times for every MAP step.

More flexibility is also available by defining how each mini-batch is
processed by the inference algorithm. The following piece of code is
equivalent to the above one,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pca} \PYG{o}{=} \PYG{n}{ProbModel}\PYG{p}{(}\PYG{n+nb}{vars} \PYG{o}{=} \PYG{p}{[}\PYG{n}{mu}\PYG{p}{,}\PYG{n}{w\PYGZus{}n}\PYG{p}{,}\PYG{n}{x\PYGZus{}n}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{q\PYGZus{}mu} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{inference}\PYG{o}{.}\PYG{n}{Q}\PYG{o}{.}\PYG{n}{PointMass}\PYG{p}{(}\PYG{n}{bind} \PYG{o}{=} \PYG{n}{mu}\PYG{p}{,} \PYG{n}{initializer}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{zeroes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{q\PYGZus{}w\PYGZus{}n} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{inference}\PYG{o}{.}\PYG{n}{Q}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{bind} \PYG{o}{=} \PYG{n}{w\PYGZus{}n}\PYG{p}{,} \PYG{n}{initializer}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{random\PYGZus{}unifrom}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{qlocal} \PYG{o}{=} \PYG{n}{QModel}\PYG{p}{(}\PYG{n+nb}{vars} \PYG{o}{=} \PYG{p}{[}\PYG{n}{q\PYGZus{}w\PYGZus{}n}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{qglobal} \PYG{o}{=} \PYG{n}{QModel}\PYG{p}{(}\PYG{n+nb}{vars} \PYG{o}{=} \PYG{p}{[}\PYG{n}{mu}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{infkl\PYGZus{}qp} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{inference}\PYG{o}{.}\PYG{n}{KLqp}\PYG{p}{(}\PYG{n}{Q} \PYG{o}{=} \PYG{n}{qlocal}\PYG{p}{,} \PYG{n}{optimizer} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sgd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{innerIter} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{infMAP} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{inference}\PYG{o}{.}\PYG{n}{MAP}\PYG{p}{(}\PYG{n}{Q} \PYG{o}{=} \PYG{n}{qglobal}\PYG{p}{,} \PYG{n}{optimizer} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sgd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{emAlg} \PYG{o}{=} \PYG{k}{lambda} \PYG{p}{(}\PYG{n}{infMethod}\PYG{p}{,} \PYG{n}{dataBatch}\PYG{p}{)}\PYG{p}{:}
   \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}
       \PYG{n}{infMethod}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{n}{data} \PYG{o}{=} \PYG{n}{dataBatch}\PYG{p}{)}

   \PYG{n}{infMethod}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{n}{data} \PYG{o}{=} \PYG{n}{dataBatch}\PYG{p}{)}
   \PYG{k}{return}

\PYG{n}{pca}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{infMethod} \PYG{o}{=} \PYG{p}{[}\PYG{n}{infkl\PYGZus{}qp}\PYG{p}{,}\PYG{n}{infMAP}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ingAlg} \PYG{o}{=} \PYG{n}{emAlg}\PYG{p}{)}

\PYG{n}{pca}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{x\PYGZus{}train}\PYG{p}{,} \PYG{n}{EPOCHS} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{posterior\PYGZus{}mu} \PYG{o}{=} \PYG{n}{pca}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{n}{mu}\PYG{p}{)}
\end{sphinxVerbatim}

Have a look again at Inference Zoo to explore other complex
compositional options.


\section{Supported Inference Methods}
\label{\detokenize{notes/guideinference:supported-inference-methods}}

\chapter{Guide to Model Validation}
\label{\detokenize{notes/guidevalidation::doc}}\label{\detokenize{notes/guidevalidation:guide-to-model-validation}}
\begin{sphinxadmonition}{note}{Note:}
not implemented yet
\end{sphinxadmonition}

Model validation try to assess how faifhfully the inferered
probabilistic model represents and explain the observed data.

The main tool for model validation consists on analyzing the posterior
predictive distribution,

\(p(y_{test}, x_{test}|y_{train}, x_{train}) = \int p(y_{test}, x_{test}|z,\theta)p(z,\theta|y_{train}, x_{train}) dzd\theta\)

This posterior predictive distribution can be used to measure how well
the model fits an independent dataset using the test marginal
log-likelihood, \(\ln p(y_{test}, x_{test}|y_{train}, x_{train})\),

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{log\PYGZus{}like} \PYG{o}{=} \PYG{n}{probmodel}\PYG{o}{.}\PYG{n}{evaluate}\PYG{p}{(}\PYG{n}{test\PYGZus{}data}\PYG{p}{,} \PYG{n}{metrics} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log\PYGZus{}likelihood}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

In other cases, we may need to evalute the predictive capacity of the
model with respect to some target variable \(y\),

\(p(y_{test}|x_{test}, y_{train}, x_{train}) = \int p(y_{test}|x_{test},z,\theta)p(z,\theta|y_{train}, x_{train}) dzd\theta\)

So the metrics can be computed with respect to this target variable by
using the ‘targetvar’ argument,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{log\PYGZus{}like}\PYG{p}{,} \PYG{n}{accuracy}\PYG{p}{,} \PYG{n}{mse} \PYG{o}{=} \PYG{n}{probmodel}\PYG{o}{.}\PYG{n}{evaluate}\PYG{p}{(}\PYG{n}{test\PYGZus{}data}\PYG{p}{,} \PYG{n}{targetvar} \PYG{o}{=} \PYG{n}{y}\PYG{p}{,} \PYG{n}{metrics} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log\PYGZus{}likelihood}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{accuracy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mse}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

So, the log-likelihood metric as well as the accuracy and the mean
square error metric are computed by using the predictive posterior
\(p(y_{test}|x_{test}, y_{train}, x_{train})\).

Custom evaluation metrics can also be defined,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{mean\PYGZus{}absolute\PYGZus{}error}\PYG{p}{(}\PYG{n}{posterior}\PYG{p}{,} \PYG{n}{observations}\PYG{p}{,} \PYG{n}{weights}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{predictions} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{map\PYGZus{}fn}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x} \PYG{p}{:} \PYG{n}{x}\PYG{o}{.}\PYG{n}{getMean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{posterior}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{metrics}\PYG{o}{.}\PYG{n}{mean\PYGZus{}absolute\PYGZus{}error}\PYG{p}{(}\PYG{n}{observations}\PYG{p}{,} \PYG{n}{predictions}\PYG{p}{,} \PYG{n}{weights}\PYG{p}{)}

\PYG{n}{mse}\PYG{p}{,} \PYG{n}{mae} \PYG{o}{=} \PYG{n}{probmodel}\PYG{o}{.}\PYG{n}{evaluate}\PYG{p}{(}\PYG{n}{test\PYGZus{}data}\PYG{p}{,} \PYG{n}{targetvar} \PYG{o}{=} \PYG{n}{y}\PYG{p}{,} \PYG{n}{metrics} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mse}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{mean\PYGZus{}absolute\PYGZus{}error}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}


\chapter{Guide to Data Handling}
\label{\detokenize{notes/guidedata::doc}}\label{\detokenize{notes/guidedata:guide-to-data-handling}}
InferPy leverages existing \sphinxhref{https://pandas.pydata.org}{Pandas} functionality for reading data. As a consequence, InferPy can
learn from datasets in any file format handled by Pandas. This is possible because the method
\sphinxcode{inferpy.ProbModel.fit(data)} accepts as input argument a Pandas DataFrame.

In the following code fragment, an example of learning a model from a CVS file is shown:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{inferpy} \PYG{k+kn}{as} \PYG{n+nn}{inf}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k+kn}{as} \PYG{n+nn}{pd}


\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{inferpy/datasets/test.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{N} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}

\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m}\PYG{p}{:}

    \PYG{n}{thetaX} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{)}
    \PYG{n}{thetaY} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{)}


    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{N}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{thetaX}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{x}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{thetaY}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}


\PYG{n}{m}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}

\PYG{n}{m}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{p}{[}\PYG{n}{thetaX}\PYG{p}{,} \PYG{n}{thetaY}\PYG{p}{]}\PYG{p}{)}



\end{sphinxVerbatim}


\chapter{Probabilistic Model Zoo}
\label{\detokenize{notes/probzoo:probabilistic-model-zoo}}\label{\detokenize{notes/probzoo:proobzoo}}\label{\detokenize{notes/probzoo::doc}}
In this section, we present the code for implementing some models in Inferpy. The corresponding code
in Edward can be found in the  \sphinxhref{inf\_vs\_ed.html}{Inferpy vs Edward} section.


\section{Bayesian Linear Regression}
\label{\detokenize{notes/probzoo:bayesian-linear-regression}}
Graphically, a (Bayesian) linear regression can be defined as follows,

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=1.0]{{linear_regression}.png}
\caption{Bayesian Linear Regression}\label{\detokenize{notes/probzoo:id1}}\end{figure}

The InferPy code for this model is shown below,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{edward} \PYG{k+kn}{as} \PYG{n+nn}{ed}
\PYG{k+kn}{import} \PYG{n+nn}{inferpy} \PYG{k+kn}{as} \PYG{n+nn}{inf}
\PYG{k+kn}{from} \PYG{n+nn}{inferpy.models} \PYG{k+kn}{import} \PYG{n}{Normal}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k+kn}{as} \PYG{n+nn}{np}

\PYG{n}{d}\PYG{p}{,} \PYG{n}{N} \PYG{o}{=}  \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{20000}

\PYG{c+c1}{\PYGZsh{} model definition}
\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m}\PYG{p}{:}

    \PYG{c+c1}{\PYGZsh{}define the weights}
    \PYG{n}{w0} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{w} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} define the generative model}
    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{N}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{w0} \PYG{o}{+} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} toy data generation}
\PYG{n}{x\PYGZus{}train} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{y\PYGZus{}train} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{x\PYGZus{}train}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mf}{0.1}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{p}{(}\PYG{n}{d}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)} \PYGZbs{}
          \PYG{o}{+} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}


\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{o}{.}\PYG{n}{name}\PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{,} \PYG{n}{y}\PYG{o}{.}\PYG{n}{name}\PYG{p}{:} \PYG{n}{y\PYGZus{}train}\PYG{p}{\PYGZcb{}}


\PYG{c+c1}{\PYGZsh{} compile and fit the model with training data}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}

\PYG{k}{print}\PYG{p}{(}\PYG{n}{m}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{p}{[}\PYG{n}{w}\PYG{p}{,} \PYG{n}{w0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}


\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{Bayesian Logistic Regression}
\label{\detokenize{notes/probzoo:bayesian-logistic-regression}}
Graphically, a (Bayesian) logistic regression can be defined as follows,

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=1.0]{{logistic_regression}.png}
\caption{Bayesian Linear Regression}\label{\detokenize{notes/probzoo:id2}}\end{figure}

The InferPy code for this model is shown below,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{edward} \PYG{k+kn}{as} \PYG{n+nn}{ed}
\PYG{k+kn}{import} \PYG{n+nn}{inferpy} \PYG{k+kn}{as} \PYG{n+nn}{inf}
\PYG{k+kn}{from} \PYG{n+nn}{inferpy.models} \PYG{k+kn}{import} \PYG{n}{Normal}\PYG{p}{,} \PYG{n}{Bernoulli}\PYG{p}{,} \PYG{n}{Categorical}

\PYG{n}{d}\PYG{p}{,} \PYG{n}{N} \PYG{o}{=}  \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{500}

\PYG{c+c1}{\PYGZsh{} model definition}
\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m}\PYG{p}{:}

    \PYG{c+c1}{\PYGZsh{}define the weights}
    \PYG{n}{w0} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{w} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} define the generative model}
    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{N}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{Bernoulli}\PYG{p}{(}\PYG{n}{logits}\PYG{o}{=}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{w}\PYG{p}{)}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} toy data generation}
\PYG{n}{x\PYGZus{}train} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{y\PYGZus{}train} \PYG{o}{=} \PYG{n}{Bernoulli}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{l+m+mf}{0.4}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{o}{.}\PYG{n}{name}\PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{,} \PYG{n}{y}\PYG{o}{.}\PYG{n}{name}\PYG{p}{:} \PYG{n}{y\PYGZus{}train}\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} compile and fit the model with training data}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}

\PYG{k}{print}\PYG{p}{(}\PYG{n}{m}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{p}{[}\PYG{n}{w}\PYG{p}{,} \PYG{n}{w0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}




\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{Bayesian Multinomial Logistic Regression}
\label{\detokenize{notes/probzoo:bayesian-multinomial-logistic-regression}}
Graphically, a (Bayesian) multinomial logistic regression can be defined as follows,

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=1.0]{{multinomial_logistic_regression}.png}
\caption{Bayesian Linear Regression}\label{\detokenize{notes/probzoo:id3}}\end{figure}

The InferPy code for this model is shown below,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{edward} \PYG{k+kn}{as} \PYG{n+nn}{ed}
\PYG{k+kn}{import} \PYG{n+nn}{inferpy} \PYG{k+kn}{as} \PYG{n+nn}{inf}
\PYG{k+kn}{from} \PYG{n+nn}{inferpy.models} \PYG{k+kn}{import} \PYG{n}{Normal}\PYG{p}{,} \PYG{n}{Bernoulli}\PYG{p}{,} \PYG{n}{Categorical}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k+kn}{as} \PYG{n+nn}{np}

\PYG{n}{d}\PYG{p}{,} \PYG{n}{N} \PYG{o}{=}  \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{500}

\PYG{c+c1}{\PYGZsh{}number of classes}
\PYG{n}{K} \PYG{o}{=} \PYG{l+m+mi}{3}

\PYG{c+c1}{\PYGZsh{} model definition}
\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m}\PYG{p}{:}

    \PYG{c+c1}{\PYGZsh{}define the weights}
    \PYG{n}{w0} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{K}\PYG{p}{)}

    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{K}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{w} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} define the generative model}
    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{N}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{Bernoulli}\PYG{p}{(}\PYG{n}{logits} \PYG{o}{=} \PYG{n}{w0} \PYG{o}{+} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{w}\PYG{p}{,} \PYG{n}{transpose\PYGZus{}b}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}



\PYG{c+c1}{\PYGZsh{} toy data generation}
\PYG{n}{x\PYGZus{}train} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{y\PYGZus{}train} \PYG{o}{=} \PYG{n}{Bernoulli}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{n}{K}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{o}{.}\PYG{n}{name}\PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{,} \PYG{n}{y}\PYG{o}{.}\PYG{n}{name}\PYG{p}{:} \PYG{n}{y\PYGZus{}train}\PYG{p}{\PYGZcb{}}


\PYG{c+c1}{\PYGZsh{} compile and fit the model with training data}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}

\PYG{k}{print}\PYG{p}{(}\PYG{n}{m}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{p}{[}\PYG{n}{w}\PYG{p}{,} \PYG{n}{w0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}


\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{Mixture of Gaussians}
\label{\detokenize{notes/probzoo:mixture-of-gaussians}}
Graphically, a Mixture of Gaussians can be defined as follows,

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=1.0]{{MoG}.png}
\caption{Bayesian Linear Regression}\label{\detokenize{notes/probzoo:id4}}\end{figure}

The InferPy code for this model is shown below,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{edward} \PYG{k+kn}{as} \PYG{n+nn}{ed}
\PYG{k+kn}{import} \PYG{n+nn}{inferpy} \PYG{k+kn}{as} \PYG{n+nn}{inf}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k+kn}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow} \PYG{k+kn}{as} \PYG{n+nn}{tf}



\PYG{n}{K}\PYG{p}{,} \PYG{n}{d}\PYG{p}{,} \PYG{n}{N}\PYG{p}{,} \PYG{n}{T} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{l+m+mi}{5000}


\PYG{c+c1}{\PYGZsh{} toy data generation}
\PYG{n}{x\PYGZus{}train} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{,}
                     \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{l+m+mi}{700}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{} Inferpy \PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}}


\PYG{c+c1}{\PYGZsh{} model definition}
\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m}\PYG{p}{:}

    \PYG{c+c1}{\PYGZsh{} prior distributions}
    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{K}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{mu} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}
        \PYG{n}{sigma} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{InverseGamma}\PYG{p}{(}\PYG{n}{concentration}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{rate}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{,}\PYG{p}{)}
    \PYG{n}{p} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Dirichlet}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{K}\PYG{p}{)}\PYG{o}{/}\PYG{n}{K}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} define the generative model}
    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{N}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{z} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs} \PYG{o}{=} \PYG{n}{p}\PYG{p}{)}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{mu}\PYG{p}{[}\PYG{n}{z}\PYG{p}{]}\PYG{p}{,} \PYG{n}{sigma}\PYG{p}{[}\PYG{n}{z}\PYG{p}{]}\PYG{p}{,}\PYG{n}{observed}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} compile and fit the model with training data}
\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{\PYGZcb{}}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{infMethod}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MCMC}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} print the posterior}
\PYG{k}{print}\PYG{p}{(}\PYG{n}{m}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{n}{mu}\PYG{p}{)}\PYG{p}{)}

\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{Linear Factor Model (PCA)}
\label{\detokenize{notes/probzoo:linear-factor-model-pca}}\begin{description}
\item[{A linear factor model allows to perform principal component analysis (PCA). Graphically,}] \leavevmode
it can be defined as follows,

\end{description}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=1.0]{{pca}.png}
\caption{Linear Factor Model (PCA)}\label{\detokenize{notes/probzoo:id5}}\end{figure}

The InferPy code for this model is shown below,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{edward} \PYG{k+kn}{as} \PYG{n+nn}{ed}
\PYG{k+kn}{import} \PYG{n+nn}{inferpy} \PYG{k+kn}{as} \PYG{n+nn}{inf}

\PYG{n}{K}\PYG{p}{,} \PYG{n}{d}\PYG{p}{,} \PYG{n}{N} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{200}

\PYG{c+c1}{\PYGZsh{} model definition}
\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{}define the weights}
    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{K}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{w} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} define the generative model}
    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{N}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{z} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{K}\PYG{p}{)}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{z}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{,}
                               \PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} toy data generation}
\PYG{n}{x\PYGZus{}train} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{o}{.}\PYG{n}{name}\PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{\PYGZcb{}}


\PYG{c+c1}{\PYGZsh{} compile and fit the model with training data}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}extract the hidden representation from a set of observations}
\PYG{n}{hidden\PYGZus{}encoding} \PYG{o}{=} \PYG{n}{m}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}
\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{PCA with ARD Prior (PCA)}
\label{\detokenize{notes/probzoo:pca-with-ard-prior-pca}}
Similarly to the previous model, the PCA with ARD Prior can be graphically defined as follows,

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=1.0]{{pca_ard}.png}
\caption{PCA with ARD Prior}\label{\detokenize{notes/probzoo:id6}}\end{figure}

Its code in InferPy is shown below,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{edward} \PYG{k+kn}{as} \PYG{n+nn}{ed}
\PYG{k+kn}{import} \PYG{n+nn}{inferpy} \PYG{k+kn}{as} \PYG{n+nn}{inf}
\PYG{k+kn}{from} \PYG{n+nn}{inferpy.models} \PYG{k+kn}{import} \PYG{n}{Normal}\PYG{p}{,} \PYG{n}{InverseGamma}

\PYG{n}{K}\PYG{p}{,} \PYG{n}{d}\PYG{p}{,} \PYG{n}{N} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{200}

\PYG{c+c1}{\PYGZsh{} model definition}
\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{}define the weights}
    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{K}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{w} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}

    \PYG{n}{sigma} \PYG{o}{=} \PYG{n}{InverseGamma}\PYG{p}{(}\PYG{l+m+mf}{1.0}\PYG{p}{,}\PYG{l+m+mf}{1.0}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} define the generative model}
    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{N}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{z} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{K}\PYG{p}{)}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{z}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{,}
                   \PYG{n}{sigma}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} toy data generation}
\PYG{n}{x\PYGZus{}train} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{o}{.}\PYG{n}{name}\PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{\PYGZcb{}}


\PYG{c+c1}{\PYGZsh{} compile and fit the model with training data}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}extract the hidden representation from a set of observations}
\PYG{n}{hidden\PYGZus{}encoding} \PYG{o}{=} \PYG{n}{m}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}

\end{sphinxVerbatim}


\chapter{Inferpy vs Edward}
\label{\detokenize{notes/inf_vs_ed:inferpy-vs-edward}}\label{\detokenize{notes/inf_vs_ed:inf-vs-ed}}\label{\detokenize{notes/inf_vs_ed::doc}}
This section shows the equivalent Edward code for those models in \sphinxhref{probzoo.html}{Probabilistic Model Zoo} section.


\section{Bayesian Linear Regression}
\label{\detokenize{notes/inf_vs_ed:bayesian-linear-regression}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} model definition}
\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{model}\PYG{p}{:}

    \PYG{c+c1}{\PYGZsh{} define the weights}
    \PYG{n}{w0} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{w} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} define the generative model}
    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{N}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{w0} \PYG{o}{+} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} compile and fit the model with training data}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{,} \PYG{n}{y}\PYG{p}{:} \PYG{n}{y\PYGZus{}train}\PYG{p}{\PYGZcb{}}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} print the posterior distributions}
\end{sphinxVerbatim}


\section{Gaussian Mixture}
\label{\detokenize{notes/inf_vs_ed:gaussian-mixture}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

    \PYG{c+c1}{\PYGZsh{} define the generative model}
    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{N}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{z} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{n}{p}\PYG{p}{)}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{mu}\PYG{p}{[}\PYG{n}{z}\PYG{p}{]}\PYG{p}{,} \PYG{n}{sigma}\PYG{p}{[}\PYG{n}{z}\PYG{p}{]}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{n}{d}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} compile and fit the model with training data}
\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{\PYGZcb{}}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{infMethod}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MCMC}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} print the posterior}
\PYG{k}{print}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{n}{mu}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{} Edward \PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}}



\PYG{c+c1}{\PYGZsh{} model definition}

\PYG{c+c1}{\PYGZsh{} prior distributions}
\PYG{n}{p} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Dirichlet}\PYG{p}{(}\PYG{n}{concentration}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{K}\PYG{p}{)} \PYG{o}{/} \PYG{n}{K}\PYG{p}{)}
\PYG{n}{mu} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,} \PYG{n}{d}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{sigma} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{InverseGamma}\PYG{p}{(}\PYG{n}{concentration}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{n}{rate}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,} \PYG{n}{d}\PYG{p}{]}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} define the generative model}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{logits}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{l+m+mf}{1.0} \PYG{o}{\PYGZhy{}} \PYG{n}{p}\PYG{p}{)}\PYG{p}{,} \PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{gather}\PYG{p}{(}\PYG{n}{mu}\PYG{p}{,} \PYG{n}{z}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{gather}\PYG{p}{(}\PYG{n}{sigma}\PYG{p}{,} \PYG{n}{z}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} compile and fit the model with training data}
\PYG{n}{qp} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Empirical}\PYG{p}{(}\PYG{n}{params}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}variable}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{qp/params}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{[}\PYG{n}{T}\PYG{p}{,} \PYG{n}{K}\PYG{p}{]}\PYG{p}{,}
                                                \PYG{n}{initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{constant\PYGZus{}initializer}\PYG{p}{(}\PYG{l+m+mf}{1.0} \PYG{o}{/} \PYG{n}{K}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{qmu} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Empirical}\PYG{p}{(}\PYG{n}{params}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}variable}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{qmu/params}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{[}\PYG{n}{T}\PYG{p}{,} \PYG{n}{K}\PYG{p}{,} \PYG{n}{d}\PYG{p}{]}\PYG{p}{,}
                                                 \PYG{n}{initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros\PYGZus{}initializer}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{qsigma} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Empirical}\PYG{p}{(}\PYG{n}{params}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}variable}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{qsigma/params}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{[}\PYG{n}{T}\PYG{p}{,} \PYG{n}{K}\PYG{p}{,} \PYG{n}{d}\PYG{p}{]}\PYG{p}{,}
                                                    \PYG{n}{initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones\PYGZus{}initializer}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{qz} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Empirical}\PYG{p}{(}\PYG{n}{params}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}variable}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{qz/params}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{[}\PYG{n}{T}\PYG{p}{,} \PYG{n}{N}\PYG{p}{]}\PYG{p}{,}
                                                \PYG{n}{initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros\PYGZus{}initializer}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
                                                \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{int32}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{gp} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Dirichlet}\PYG{p}{(}\PYG{n}{concentration}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{K}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{gmu} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,} \PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,} \PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{gsigma} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{InverseGamma}\PYG{p}{(}\PYG{n}{concentration}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,} \PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{rate}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,} \PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{gz} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{logits}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{N}\PYG{p}{,} \PYG{n}{K}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{inference} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{MetropolisHastings}\PYG{p}{(}
    \PYG{n}{latent\PYGZus{}vars}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{p}\PYG{p}{:} \PYG{n}{qp}\PYG{p}{,} \PYG{n}{mu}\PYG{p}{:} \PYG{n}{qmu}\PYG{p}{,} \PYG{n}{sigma}\PYG{p}{:} \PYG{n}{qsigma}\PYG{p}{,} \PYG{n}{z}\PYG{p}{:} \PYG{n}{qz}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{n}{proposal\PYGZus{}vars}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{p}\PYG{p}{:} \PYG{n}{gp}\PYG{p}{,} \PYG{n}{mu}\PYG{p}{:} \PYG{n}{gmu}\PYG{p}{,} \PYG{n}{sigma}\PYG{p}{:} \PYG{n}{gsigma}\PYG{p}{,} \PYG{n}{z}\PYG{p}{:} \PYG{n}{gz}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{n}{data}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{inference}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} print the posterior}
\PYG{k}{print}\PYG{p}{(}\PYG{n}{qmu}\PYG{o}{.}\PYG{n}{params}\PYG{o}{.}\PYG{n}{eval}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\end{sphinxVerbatim}


\section{Logistic Regression}
\label{\detokenize{notes/inf_vs_ed:logistic-regression}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\PYG{c+c1}{\PYGZsh{} define the weights}
\PYG{n}{w0} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{w} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{d}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{d}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} define the generative model}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{N}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{n}{N}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Bernoulli}\PYG{p}{(}\PYG{n}{logits}\PYG{o}{=}\PYG{n}{ed}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{w}\PYG{p}{)} \PYG{o}{+} \PYG{n}{w0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} compile and fit the model with training data}
\PYG{n}{qw} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{random\PYGZus{}normal}\PYG{p}{(}\PYG{p}{[}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
                      \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{softplus}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{random\PYGZus{}normal}\PYG{p}{(}\PYG{p}{[}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{qw0} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{random\PYGZus{}normal}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
                       \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{softplus}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{random\PYGZus{}normal}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{inference} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{KLqp}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{w}\PYG{p}{:} \PYG{n}{qw}\PYG{p}{,} \PYG{n}{w0}\PYG{p}{:} \PYG{n}{qw0}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{,} \PYG{n}{y}\PYG{p}{:} \PYG{n}{y\PYGZus{}train}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{inference}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} print the posterior distributions}
\PYG{k}{print}\PYG{p}{(}\PYG{p}{[}\PYG{n}{qw}\PYG{o}{.}\PYG{n}{loc}\PYG{o}{.}\PYG{n}{eval}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{qw0}\PYG{o}{.}\PYG{n}{loc}\PYG{o}{.}\PYG{n}{eval}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}

\end{sphinxVerbatim}


\section{Multinomial Logistic Regression}
\label{\detokenize{notes/inf_vs_ed:multinomial-logistic-regression}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\PYG{c+c1}{\PYGZsh{} define the weights}
\PYG{n}{w0} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{K}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{K}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{w} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} define the generative model}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{N}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{n}{N}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{w0} \PYG{o}{+} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{w}\PYG{p}{,} \PYG{n}{transpose\PYGZus{}b}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{n}{N}\PYG{p}{,}\PYG{n}{K}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} compile and fit the model with training data}
\PYG{n}{qw} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{random\PYGZus{}normal}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
                      \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{softplus}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{random\PYGZus{}normal}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{qw0} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{random\PYGZus{}normal}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
                       \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{softplus}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{random\PYGZus{}normal}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{inference} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{KLqp}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{w}\PYG{p}{:} \PYG{n}{qw}\PYG{p}{,} \PYG{n}{w0}\PYG{p}{:} \PYG{n}{qw0}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{,} \PYG{n}{y}\PYG{p}{:} \PYG{n}{y\PYGZus{}train}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{inference}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} print the posterior distributions}
\PYG{k}{print}\PYG{p}{(}\PYG{p}{[}\PYG{n}{qw}\PYG{o}{.}\PYG{n}{loc}\PYG{o}{.}\PYG{n}{eval}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{qw0}\PYG{o}{.}\PYG{n}{loc}\PYG{o}{.}\PYG{n}{eval}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}

\end{sphinxVerbatim}


\section{Linear Factor Model (PCA)}
\label{\detokenize{notes/inf_vs_ed:linear-factor-model-pca}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\PYG{c+c1}{\PYGZsh{} define the weights}
\PYG{n}{w} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} define the generative model}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{N}\PYG{p}{,}\PYG{n}{K}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{n}{N}\PYG{p}{,}\PYG{n}{K}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{z}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{n}{N}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} compile and fit the model with training data}
\PYG{n}{qw} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{random\PYGZus{}normal}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
                      \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{softplus}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{random\PYGZus{}normal}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}


\PYG{n}{inference} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{KLqp}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{w}\PYG{p}{:} \PYG{n}{qw}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{inference}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} print the posterior distributions}
\PYG{k}{print}\PYG{p}{(}\PYG{p}{[}\PYG{n}{qw}\PYG{o}{.}\PYG{n}{loc}\PYG{o}{.}\PYG{n}{eval}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}

\end{sphinxVerbatim}


\section{PCA with ARD Prior (PCA)}
\label{\detokenize{notes/inf_vs_ed:pca-with-ard-prior-pca}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]

\PYG{c+c1}{\PYGZsh{} define the weights}
\PYG{n}{w} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{sigma} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{InverseGamma}\PYG{p}{(}\PYG{l+m+mf}{1.}\PYG{p}{,}\PYG{l+m+mf}{1.}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} define the generative model}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{N}\PYG{p}{,}\PYG{n}{K}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{n}{N}\PYG{p}{,}\PYG{n}{K}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{z}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{sigma}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} compile and fit the model with training data}
\PYG{n}{qw} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{random\PYGZus{}normal}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
                      \PYG{n}{scale}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{softplus}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{random\PYGZus{}normal}\PYG{p}{(}\PYG{p}{[}\PYG{n}{K}\PYG{p}{,}\PYG{n}{d}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}


\PYG{n}{inference} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{KLqp}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{w}\PYG{p}{:} \PYG{n}{qw}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{inference}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} print the posterior distributions}
\PYG{k}{print}\PYG{p}{(}\PYG{p}{[}\PYG{n}{qw}\PYG{o}{.}\PYG{n}{loc}\PYG{o}{.}\PYG{n}{eval}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}

\end{sphinxVerbatim}


\chapter{inferpy package}
\label{\detokenize{modules/inferpy:inferpy-package}}\label{\detokenize{modules/inferpy::doc}}

\section{Subpackages}
\label{\detokenize{modules/inferpy:subpackages}}

\subsection{inferpy.criticism package}
\label{\detokenize{modules/inferpy.criticism::doc}}\label{\detokenize{modules/inferpy.criticism:inferpy-criticism-package}}

\subsubsection{Submodules}
\label{\detokenize{modules/inferpy.criticism:submodules}}

\subsubsection{inferpy.criticism.evaluate module}
\label{\detokenize{modules/inferpy.criticism:module-inferpy.criticism.evaluate}}\label{\detokenize{modules/inferpy.criticism:inferpy-criticism-evaluate-module}}\index{inferpy.criticism.evaluate (module)}
Module with the functionality for evaluating the InferPy models
\index{ALLOWED\_METRICS (in module inferpy.criticism.evaluate)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.criticism:inferpy.criticism.evaluate.ALLOWED_METRICS}}\pysigline{\sphinxcode{inferpy.criticism.evaluate.}\sphinxbfcode{ALLOWED\_METRICS}\sphinxbfcode{ = {[}'binary\_accuracy', 'categorical\_accuracy', 'sparse\_categorical\_accuracy', 'log\_loss', 'binary\_crossentropy', 'categorical\_crossentropy', 'sparse\_categorical\_crossentropy', 'hinge', 'squared\_hinge', 'mse', 'MSE', 'mean\_squared\_error', 'mae', 'MAE', 'mean\_absolute\_error', 'mape', 'MAPE', 'mean\_absolute\_percentage\_error', 'msle', 'MSLE', 'mean\_squared\_logarithmic\_error', 'poisson', 'cosine', 'cosine\_proximity', 'log\_lik', 'log\_likelihood'{]}}}
List with all the allowed metrics for evaluation

\end{fulllineitems}

\index{evaluate() (in module inferpy.criticism.evaluate)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.criticism:inferpy.criticism.evaluate.evaluate}}\pysiglinewithargsret{\sphinxcode{inferpy.criticism.evaluate.}\sphinxbfcode{evaluate}}{\emph{metrics}, \emph{data}, \emph{n\_samples=500}, \emph{output\_key=None}, \emph{seed=None}}{}
Evaluate a fitted inferpy model using a set of metrics. This function
encapsulates the equivalent Edward one.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{metrics} \textendash{} list of str indicating the metrics or sccore functions to be used.

\end{description}\end{quote}

An example of use:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} evaluate the predicted data y=y\PYGZus{}pred given that x=x\PYGZus{}test}
\PYG{n}{mse} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{evaluate}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean\PYGZus{}squared\PYGZus{}error}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{p}{:} \PYG{n}{x\PYGZus{}test}\PYG{p}{,} \PYG{n}{y}\PYG{p}{:} \PYG{n}{y\PYGZus{}pred}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{output\PYGZus{}key}\PYG{o}{=}\PYG{n}{y}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
A list of evaluations or a single evaluation.

\item[{Return type}] \leavevmode
list of float or float

\item[{Raises}] \leavevmode
\sphinxcode{NotImplementedError} \textendash{} If an input metric does not match an implemented metric.

\end{description}\end{quote}

\end{fulllineitems}



\subsection{inferpy.inferences package}
\label{\detokenize{modules/inferpy.inferences::doc}}\label{\detokenize{modules/inferpy.inferences:inferpy-inferences-package}}

\subsubsection{Submodules}
\label{\detokenize{modules/inferpy.inferences:submodules}}

\subsubsection{inferpy.inferences.inference module}
\label{\detokenize{modules/inferpy.inferences:inferpy-inferences-inference-module}}\label{\detokenize{modules/inferpy.inferences:module-inferpy.inferences.inference}}\index{inferpy.inferences.inference (module)}
Module with the functionality related to inference methods.
\index{INF\_METHODS (in module inferpy.inferences.inference)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.inference.INF_METHODS}}\pysigline{\sphinxcode{inferpy.inferences.inference.}\sphinxbfcode{INF\_METHODS}\sphinxbfcode{ = {[}'KLpq', 'KLqp', 'Laplace', 'ReparameterizationEntropyKLqp', 'ReparameterizationKLKLqp', 'ReparameterizationKLqp', 'ScoreEntropyKLqp', 'ScoreKLKLqp', 'ScoreKLqp', 'ScoreRBKLqp', 'WakeSleep', 'MetropolisHastings'{]}}}
List with all the alowed implemented inference methods

\end{fulllineitems}

\index{INF\_METHODS\_ALIAS (in module inferpy.inferences.inference)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.inference.INF_METHODS_ALIAS}}\pysigline{\sphinxcode{inferpy.inferences.inference.}\sphinxbfcode{INF\_METHODS\_ALIAS}\sphinxbfcode{ = \{'Variational': 'KLqp', 'MCMC': 'MetropolisHastings'\}}}
Aliases for some of the inference methods

\end{fulllineitems}



\subsubsection{inferpy.inferences.qmodel module}
\label{\detokenize{modules/inferpy.inferences:inferpy-inferences-qmodel-module}}\label{\detokenize{modules/inferpy.inferences:module-inferpy.inferences.qmodel}}\index{inferpy.inferences.qmodel (module)}
Module with the required functionality for implementing the Q-models and Q-distributions used by
some inference algorithms.
\index{Qmodel (class in inferpy.inferences.qmodel)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.inferences.qmodel.}\sphinxbfcode{Qmodel}}{\emph{varlist}}{}
Bases: \sphinxcode{object}

Class implementing a Q model

A Q model approximates the posterior distribution of a probabilistic model P .
In the ‘Q’ model we should include a q distribution for every non observed variable in the ‘P’ model.
Otherwise, an error will be raised during model compilation.

An example of use:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{edward} \PYG{k}{as} \PYG{n+nn}{ed}
\PYG{k+kn}{import} \PYG{n+nn}{inferpy} \PYG{k}{as} \PYG{n+nn}{inf}


\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{} learning a 2 parameters of 1\PYGZhy{}dim from 2\PYGZhy{}dim data}

\PYG{n}{N} \PYG{o}{=} \PYG{l+m+mi}{50}
\PYG{n}{sampling\PYGZus{}mean} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mf}{30.}\PYG{p}{,} \PYG{l+m+mf}{10.}\PYG{p}{]}
\PYG{n}{sess} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{util}\PYG{o}{.}\PYG{n}{get\PYGZus{}session}\PYG{p}{(}\PYG{p}{)}


\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m}\PYG{p}{:}

    \PYG{n}{theta1} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{theta2} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}


    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{N}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{p}{[}\PYG{n}{theta1}\PYG{p}{,} \PYG{n}{theta2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} define the Qmodel}



\PYG{c+c1}{\PYGZsh{} define with any type}
\PYG{n}{q\PYGZus{}theta1} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{Qmodel}\PYG{o}{.}\PYG{n}{Uniform}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{)}
\PYG{n}{q\PYGZus{}theta2} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{Qmodel}\PYG{o}{.}\PYG{n}{new\PYGZus{}qvar}\PYG{p}{(}\PYG{n}{theta2}\PYG{p}{)}


\PYG{n}{qmodel} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{Qmodel}\PYG{p}{(}\PYG{p}{[}\PYG{n}{q\PYGZus{}theta1}\PYG{p}{,} \PYG{n}{q\PYGZus{}theta2}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{inf}\PYG{o}{.}\PYG{n}{Qmodel}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{)}

\PYG{n}{m}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{Q}\PYG{o}{=}\PYG{n}{qmodel}\PYG{p}{)}


\PYG{n}{x\PYGZus{}train} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{sampling\PYGZus{}mean}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{o}{.}\PYG{n}{name} \PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{\PYGZcb{}}


\PYG{n}{m}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}

\PYG{n}{m}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{base\PYGZus{}object}

\PYG{n}{m}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{n}{theta2}\PYG{p}{)}




\end{sphinxVerbatim}
\index{Bernoulli() (inferpy.inferences.qmodel.Qmodel class method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.Bernoulli}}\pysiglinewithargsret{\sphinxbfcode{classmethod }\sphinxbfcode{Bernoulli}}{\emph{v}, \emph{initializer='ones'}}{}
Creates a new q-variable of type Bernoulli
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{v} \textendash{} Inferpy p-variable

\item {} 
\sphinxstyleliteralstrong{initializer} (\sphinxstyleliteralemphasis{str}) \textendash{} indicates how the new variable should be initialized. Possible values: ‘ones’ , ‘zeroes’.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{Beta() (inferpy.inferences.qmodel.Qmodel class method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.Beta}}\pysiglinewithargsret{\sphinxbfcode{classmethod }\sphinxbfcode{Beta}}{\emph{v}, \emph{initializer='ones'}}{}
Creates a new q-variable of type Beta
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{v} \textendash{} Inferpy p-variable

\item {} 
\sphinxstyleliteralstrong{initializer} (\sphinxstyleliteralemphasis{str}) \textendash{} indicates how the new variable should be initialized. Possible values: ‘ones’ , ‘zeroes’.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{Categorical() (inferpy.inferences.qmodel.Qmodel class method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.Categorical}}\pysiglinewithargsret{\sphinxbfcode{classmethod }\sphinxbfcode{Categorical}}{\emph{v}, \emph{initializer='ones'}}{}
Creates a new q-variable of type Categorical
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{v} \textendash{} Inferpy p-variable

\item {} 
\sphinxstyleliteralstrong{initializer} (\sphinxstyleliteralemphasis{str}) \textendash{} indicates how the new variable should be initialized. Possible values: ‘ones’ , ‘zeroes’.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{Deterministic() (inferpy.inferences.qmodel.Qmodel class method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.Deterministic}}\pysiglinewithargsret{\sphinxbfcode{classmethod }\sphinxbfcode{Deterministic}}{\emph{v}, \emph{initializer='ones'}}{}
Creates a new q-variable of type Deterministic
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{v} \textendash{} Inferpy p-variable

\item {} 
\sphinxstyleliteralstrong{initializer} (\sphinxstyleliteralemphasis{str}) \textendash{} indicates how the new variable should be initialized. Possible values: ‘ones’ , ‘zeroes’.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{Dirichlet() (inferpy.inferences.qmodel.Qmodel class method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.Dirichlet}}\pysiglinewithargsret{\sphinxbfcode{classmethod }\sphinxbfcode{Dirichlet}}{\emph{v}, \emph{initializer='ones'}}{}
Creates a new q-variable of type Dirichlet
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{v} \textendash{} Inferpy p-variable

\item {} 
\sphinxstyleliteralstrong{initializer} (\sphinxstyleliteralemphasis{str}) \textendash{} indicates how the new variable should be initialized. Possible values: ‘ones’ , ‘zeroes’.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{Empirical() (inferpy.inferences.qmodel.Qmodel static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.Empirical}}\pysiglinewithargsret{\sphinxbfcode{static }\sphinxbfcode{Empirical}}{\emph{v}, \emph{n\_post\_samples=500}, \emph{initializer='ones'}}{}
\end{fulllineitems}

\index{Exponential() (inferpy.inferences.qmodel.Qmodel class method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.Exponential}}\pysiglinewithargsret{\sphinxbfcode{classmethod }\sphinxbfcode{Exponential}}{\emph{v}, \emph{initializer='ones'}}{}
Creates a new q-variable of type Exponential
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{v} \textendash{} Inferpy p-variable

\item {} 
\sphinxstyleliteralstrong{initializer} (\sphinxstyleliteralemphasis{str}) \textendash{} indicates how the new variable should be initialized. Possible values: ‘ones’ , ‘zeroes’.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{Gamma() (inferpy.inferences.qmodel.Qmodel class method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.Gamma}}\pysiglinewithargsret{\sphinxbfcode{classmethod }\sphinxbfcode{Gamma}}{\emph{v}, \emph{initializer='ones'}}{}
Creates a new q-variable of type Gamma
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{v} \textendash{} Inferpy p-variable

\item {} 
\sphinxstyleliteralstrong{initializer} (\sphinxstyleliteralemphasis{str}) \textendash{} indicates how the new variable should be initialized. Possible values: ‘ones’ , ‘zeroes’.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{InverseGamma() (inferpy.inferences.qmodel.Qmodel class method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.InverseGamma}}\pysiglinewithargsret{\sphinxbfcode{classmethod }\sphinxbfcode{InverseGamma}}{\emph{v}, \emph{initializer='ones'}}{}
Creates a new q-variable of type InverseGamma
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{v} \textendash{} Inferpy p-variable

\item {} 
\sphinxstyleliteralstrong{initializer} (\sphinxstyleliteralemphasis{str}) \textendash{} indicates how the new variable should be initialized. Possible values: ‘ones’ , ‘zeroes’.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{Laplace() (inferpy.inferences.qmodel.Qmodel class method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.Laplace}}\pysiglinewithargsret{\sphinxbfcode{classmethod }\sphinxbfcode{Laplace}}{\emph{v}, \emph{initializer='ones'}}{}
Creates a new q-variable of type Laplace
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{v} \textendash{} Inferpy p-variable

\item {} 
\sphinxstyleliteralstrong{initializer} (\sphinxstyleliteralemphasis{str}) \textendash{} indicates how the new variable should be initialized. Possible values: ‘ones’ , ‘zeroes’.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{Multinomial() (inferpy.inferences.qmodel.Qmodel class method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.Multinomial}}\pysiglinewithargsret{\sphinxbfcode{classmethod }\sphinxbfcode{Multinomial}}{\emph{v}, \emph{initializer='ones'}}{}
Creates a new q-variable of type Multinomial
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{v} \textendash{} Inferpy p-variable

\item {} 
\sphinxstyleliteralstrong{initializer} (\sphinxstyleliteralemphasis{str}) \textendash{} indicates how the new variable should be initialized. Possible values: ‘ones’ , ‘zeroes’.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{MultivariateNormalDiag() (inferpy.inferences.qmodel.Qmodel class method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.MultivariateNormalDiag}}\pysiglinewithargsret{\sphinxbfcode{classmethod }\sphinxbfcode{MultivariateNormalDiag}}{\emph{v}, \emph{initializer='ones'}}{}
Creates a new q-variable of type MultivariateNormalDiag
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{v} \textendash{} Inferpy p-variable

\item {} 
\sphinxstyleliteralstrong{initializer} (\sphinxstyleliteralemphasis{str}) \textendash{} indicates how the new variable should be initialized. Possible values: ‘ones’ , ‘zeroes’.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{Normal() (inferpy.inferences.qmodel.Qmodel class method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.Normal}}\pysiglinewithargsret{\sphinxbfcode{classmethod }\sphinxbfcode{Normal}}{\emph{v}, \emph{initializer='ones'}}{}
Creates a new q-variable of type Normal
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{v} \textendash{} Inferpy p-variable

\item {} 
\sphinxstyleliteralstrong{initializer} (\sphinxstyleliteralemphasis{str}) \textendash{} indicates how the new variable should be initialized. Possible values: ‘ones’ , ‘zeroes’.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{Poisson() (inferpy.inferences.qmodel.Qmodel class method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.Poisson}}\pysiglinewithargsret{\sphinxbfcode{classmethod }\sphinxbfcode{Poisson}}{\emph{v}, \emph{initializer='ones'}}{}
Creates a new q-variable of type Poisson
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{v} \textendash{} Inferpy p-variable

\item {} 
\sphinxstyleliteralstrong{initializer} (\sphinxstyleliteralemphasis{str}) \textendash{} indicates how the new variable should be initialized. Possible values: ‘ones’ , ‘zeroes’.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{Uniform() (inferpy.inferences.qmodel.Qmodel class method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.Uniform}}\pysiglinewithargsret{\sphinxbfcode{classmethod }\sphinxbfcode{Uniform}}{\emph{v}, \emph{initializer='ones'}}{}
Creates a new q-variable of type Uniform
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{v} \textendash{} Inferpy p-variable

\item {} 
\sphinxstyleliteralstrong{initializer} (\sphinxstyleliteralemphasis{str}) \textendash{} indicates how the new variable should be initialized. Possible values: ‘ones’ , ‘zeroes’.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{\_\_init\_\_() (inferpy.inferences.qmodel.Qmodel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{varlist}}{}
\end{fulllineitems}

\index{add\_var() (inferpy.inferences.qmodel.Qmodel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.add_var}}\pysiglinewithargsret{\sphinxbfcode{add\_var}}{\emph{v}}{}
Method for adding a new q variable

\end{fulllineitems}

\index{build\_from\_pmodel() (inferpy.inferences.qmodel.Qmodel static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.build_from_pmodel}}\pysiglinewithargsret{\sphinxbfcode{static }\sphinxbfcode{build\_from\_pmodel}}{\emph{p}, \emph{empirical=False}}{}
Initializes a Q model from a P model.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{p} \textendash{} P model of type inferpy.ProbModel.

\item {} 
\sphinxstyleliteralstrong{empirical} \textendash{} determines if q distributions will be empirical or of the same type than each p distribution.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{compatible\_var() (inferpy.inferences.qmodel.Qmodel static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.compatible_var}}\pysiglinewithargsret{\sphinxbfcode{static }\sphinxbfcode{compatible\_var}}{\emph{v}}{}
\end{fulllineitems}

\index{dict (inferpy.inferences.qmodel.Qmodel attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.dict}}\pysigline{\sphinxbfcode{dict}}
Dictionary where the keys and values are the p and q distributions respectively.

\end{fulllineitems}

\index{new\_qvar() (inferpy.inferences.qmodel.Qmodel static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.new_qvar}}\pysiglinewithargsret{\sphinxbfcode{static }\sphinxbfcode{new\_qvar}}{\emph{v}, \emph{initializer='ones'}, \emph{qvar\_inf\_module=None}, \emph{qvar\_inf\_type=None}, \emph{qvar\_ed\_type=None}, \emph{check\_observed=True}, \emph{name='qvar'}}{}
Builds an Inferpy q-variable for a p-variable.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{v} \textendash{} Inferpy variable to be approximated.

\item {} 
\sphinxstyleliteralstrong{initializer} (\sphinxstyleliteralemphasis{str}) \textendash{} indicates how the new variable should be initialized. Possible values: “ones” , “zeroes”.

\item {} 
\sphinxstyleliteralstrong{qvar\_inf\_module} (\sphinxstyleliteralemphasis{str}) \textendash{} module of the new Inferpy variable.

\item {} 
\sphinxstyleliteralstrong{qvar\_inf\_type} (\sphinxstyleliteralemphasis{str}) \textendash{} name of the new Inferpy variable.

\item {} 
\sphinxstyleliteralstrong{qvar\_ed\_type} (\sphinxstyleliteralemphasis{str}) \textendash{} full name of the encapsulated Edward variable type.

\item {} 
\sphinxstyleliteralstrong{check\_observed} (\sphinxstyleliteralemphasis{bool}) \textendash{} To check if p-variable is observed.

\end{itemize}

\item[{Returns}] \leavevmode
Inferpy variable approximating in input variable.

\end{description}\end{quote}

\end{fulllineitems}

\index{new\_qvar\_empirical() (inferpy.inferences.qmodel.Qmodel static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.new_qvar_empirical}}\pysiglinewithargsret{\sphinxbfcode{static }\sphinxbfcode{new\_qvar\_empirical}}{\emph{v}, \emph{n\_post\_samples}, \emph{initializer='ones'}, \emph{check\_observed=True}, \emph{name='qvar'}}{}
Builds an empirical Inferpy q-variable for a p-variable.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{v} \textendash{} Inferpy variable to be approximated.

\item {} 
\sphinxstyleliteralstrong{n\_post\_samples} \textendash{} number of posterior samples.

\item {} 
\sphinxstyleliteralstrong{initializer} (\sphinxstyleliteralemphasis{str}) \textendash{} indicates how the new variable should be initialized. Possible values: “ones” , “zeroes”.

\item {} 
\sphinxstyleliteralstrong{check\_observed} (\sphinxstyleliteralemphasis{bool}) \textendash{} To check if p-variable is observed.

\end{itemize}

\item[{Returns}] \leavevmode
Inferpy variable approximating in input variable. The InferPy type will be Deterministic while
the encapsulated Edward variable will be of type Empirical.

\end{description}\end{quote}

\end{fulllineitems}

\index{varlist (inferpy.inferences.qmodel.Qmodel attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.inferences:inferpy.inferences.qmodel.Qmodel.varlist}}\pysigline{\sphinxbfcode{varlist}}
list of q variables of Inferpy type

\end{fulllineitems}


\end{fulllineitems}



\subsection{inferpy.models package}
\label{\detokenize{modules/inferpy.models::doc}}\label{\detokenize{modules/inferpy.models:inferpy-models-package}}

\subsubsection{Submodules}
\label{\detokenize{modules/inferpy.models:submodules}}

\subsubsection{inferpy.models.deterministic module}
\label{\detokenize{modules/inferpy.models:module-inferpy.models.deterministic}}\label{\detokenize{modules/inferpy.models:inferpy-models-deterministic-module}}\index{inferpy.models.deterministic (module)}
The Deterministic distribution
\index{Deterministic (class in inferpy.models.deterministic)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.deterministic.Deterministic}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.deterministic.}\sphinxbfcode{Deterministic}}{\emph{loc=None}, \emph{dim=None}, \emph{observed=False}, \emph{name='Determ'}}{}
Bases: {\hyperref[\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable}]{\sphinxcrossref{\sphinxcode{inferpy.models.random\_variable.RandomVariable}}}}

Class implementing a Deterministic variable.

This allows to encapsulate any Tensor object of Edward variable. Moreover, variables
of this type can be intilially empty and later initialized. When operating with InferPy
random variables, the result is always a deterministic variable.

An example of use:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{inferpy} \PYG{k}{as} \PYG{n+nn}{inf}
\PYG{k+kn}{import} \PYG{n+nn}{edward} \PYG{k}{as} \PYG{n+nn}{ed}
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow} \PYG{k}{as} \PYG{n+nn}{tf}


\PYG{c+c1}{\PYGZsh{} empty initialized variable}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Deterministic}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{x}\PYG{o}{.}\PYG{n}{dist} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mf}{0.}\PYG{p}{,}\PYG{l+m+mf}{1.}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} deterministic variable with returning a tensor}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Deterministic}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{x}\PYG{o}{.}\PYG{n}{base\PYGZus{}object} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{constant}\PYG{p}{(}\PYG{l+m+mf}{1.}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} deterministic variable returning an InferPy variable}

\PYG{n}{a} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{b} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{a} \PYG{o}{+} \PYG{n}{b}

\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} \PYGZlt{}class \PYGZsq{}inferpy.models.deterministic.Deterministic\PYGZsq{}\PYGZgt{}}

\end{sphinxVerbatim}
\index{\_\_init\_\_() (inferpy.models.deterministic.Deterministic method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.deterministic.Deterministic.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{loc=None}, \emph{dim=None}, \emph{observed=False}, \emph{name='Determ'}}{}
Constructor for the Deterministic distribution

\end{fulllineitems}

\index{loc (inferpy.models.deterministic.Deterministic attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.deterministic.Deterministic.loc}}\pysigline{\sphinxbfcode{loc}}
Distribution parameter for the mean.

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{inferpy.models.factory module}
\label{\detokenize{modules/inferpy.models:module-inferpy.models.factory}}\label{\detokenize{modules/inferpy.models:inferpy-models-factory-module}}\index{inferpy.models.factory (module)}
This module implements all the methods for definition of each type of random variable.
For further details about all the supported distributions,
see \sphinxhref{../notes/guidemodels.html\#supported-probability-distributions}{Guide to Building Probabilistic Models} .
\index{Bernoulli (class in inferpy.models.factory)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Bernoulli}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.factory.}\sphinxbfcode{Bernoulli}}{\emph{*args}, \emph{**kwargs}}{}
Bases: {\hyperref[\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable}]{\sphinxcrossref{\sphinxcode{inferpy.models.random\_variable.RandomVariable}}}}
\index{PARAMS (inferpy.models.factory.Bernoulli attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Bernoulli.PARAMS}}\pysigline{\sphinxbfcode{PARAMS}\sphinxbfcode{ = {[}'logits', 'probs'{]}}}
\end{fulllineitems}

\index{\_\_init\_\_() (inferpy.models.factory.Bernoulli method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Bernoulli.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{*args}, \emph{**kwargs}}{}
constructor for Bernoulli

\end{fulllineitems}

\index{logits (inferpy.models.factory.Bernoulli attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Bernoulli.logits}}\pysigline{\sphinxbfcode{logits}}
property for logits

\end{fulllineitems}

\index{probs (inferpy.models.factory.Bernoulli attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Bernoulli.probs}}\pysigline{\sphinxbfcode{probs}}
property for probs

\end{fulllineitems}


\end{fulllineitems}

\index{Beta (class in inferpy.models.factory)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Beta}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.factory.}\sphinxbfcode{Beta}}{\emph{*args}, \emph{**kwargs}}{}
Bases: {\hyperref[\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable}]{\sphinxcrossref{\sphinxcode{inferpy.models.random\_variable.RandomVariable}}}}
\index{PARAMS (inferpy.models.factory.Beta attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Beta.PARAMS}}\pysigline{\sphinxbfcode{PARAMS}\sphinxbfcode{ = {[}'concentration1', 'concentration0'{]}}}
\end{fulllineitems}

\index{\_\_init\_\_() (inferpy.models.factory.Beta method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Beta.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{*args}, \emph{**kwargs}}{}
constructor for Beta

\end{fulllineitems}

\index{concentration0 (inferpy.models.factory.Beta attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Beta.concentration0}}\pysigline{\sphinxbfcode{concentration0}}
property for concentration0

\end{fulllineitems}

\index{concentration1 (inferpy.models.factory.Beta attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Beta.concentration1}}\pysigline{\sphinxbfcode{concentration1}}
property for concentration1

\end{fulllineitems}


\end{fulllineitems}

\index{Categorical (class in inferpy.models.factory)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Categorical}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.factory.}\sphinxbfcode{Categorical}}{\emph{*args}, \emph{**kwargs}}{}
Bases: {\hyperref[\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable}]{\sphinxcrossref{\sphinxcode{inferpy.models.random\_variable.RandomVariable}}}}
\index{PARAMS (inferpy.models.factory.Categorical attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Categorical.PARAMS}}\pysigline{\sphinxbfcode{PARAMS}\sphinxbfcode{ = {[}'logits', 'probs'{]}}}
\end{fulllineitems}

\index{\_\_init\_\_() (inferpy.models.factory.Categorical method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Categorical.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{*args}, \emph{**kwargs}}{}
constructor for Categorical

\end{fulllineitems}

\index{logits (inferpy.models.factory.Categorical attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Categorical.logits}}\pysigline{\sphinxbfcode{logits}}
property for logits

\end{fulllineitems}

\index{probs (inferpy.models.factory.Categorical attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Categorical.probs}}\pysigline{\sphinxbfcode{probs}}
property for probs

\end{fulllineitems}


\end{fulllineitems}

\index{Dirichlet (class in inferpy.models.factory)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Dirichlet}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.factory.}\sphinxbfcode{Dirichlet}}{\emph{*args}, \emph{**kwargs}}{}
Bases: {\hyperref[\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable}]{\sphinxcrossref{\sphinxcode{inferpy.models.random\_variable.RandomVariable}}}}
\index{PARAMS (inferpy.models.factory.Dirichlet attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Dirichlet.PARAMS}}\pysigline{\sphinxbfcode{PARAMS}\sphinxbfcode{ = {[}'concentration'{]}}}
\end{fulllineitems}

\index{\_\_init\_\_() (inferpy.models.factory.Dirichlet method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Dirichlet.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{*args}, \emph{**kwargs}}{}
constructor for Dirichlet

\end{fulllineitems}

\index{concentration (inferpy.models.factory.Dirichlet attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Dirichlet.concentration}}\pysigline{\sphinxbfcode{concentration}}
property for concentration

\end{fulllineitems}


\end{fulllineitems}

\index{Exponential (class in inferpy.models.factory)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Exponential}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.factory.}\sphinxbfcode{Exponential}}{\emph{*args}, \emph{**kwargs}}{}
Bases: {\hyperref[\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable}]{\sphinxcrossref{\sphinxcode{inferpy.models.random\_variable.RandomVariable}}}}
\index{PARAMS (inferpy.models.factory.Exponential attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Exponential.PARAMS}}\pysigline{\sphinxbfcode{PARAMS}\sphinxbfcode{ = {[}'rate'{]}}}
\end{fulllineitems}

\index{\_\_init\_\_() (inferpy.models.factory.Exponential method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Exponential.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{*args}, \emph{**kwargs}}{}
constructor for Exponential

\end{fulllineitems}

\index{rate (inferpy.models.factory.Exponential attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Exponential.rate}}\pysigline{\sphinxbfcode{rate}}
property for rate

\end{fulllineitems}


\end{fulllineitems}

\index{Gamma (class in inferpy.models.factory)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Gamma}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.factory.}\sphinxbfcode{Gamma}}{\emph{*args}, \emph{**kwargs}}{}
Bases: {\hyperref[\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable}]{\sphinxcrossref{\sphinxcode{inferpy.models.random\_variable.RandomVariable}}}}
\index{PARAMS (inferpy.models.factory.Gamma attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Gamma.PARAMS}}\pysigline{\sphinxbfcode{PARAMS}\sphinxbfcode{ = {[}'concentration', 'rate'{]}}}
\end{fulllineitems}

\index{\_\_init\_\_() (inferpy.models.factory.Gamma method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Gamma.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{*args}, \emph{**kwargs}}{}
constructor for Gamma

\end{fulllineitems}

\index{concentration (inferpy.models.factory.Gamma attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Gamma.concentration}}\pysigline{\sphinxbfcode{concentration}}
property for concentration

\end{fulllineitems}

\index{rate (inferpy.models.factory.Gamma attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Gamma.rate}}\pysigline{\sphinxbfcode{rate}}
property for rate

\end{fulllineitems}


\end{fulllineitems}

\index{InverseGamma (class in inferpy.models.factory)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.InverseGamma}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.factory.}\sphinxbfcode{InverseGamma}}{\emph{*args}, \emph{**kwargs}}{}
Bases: {\hyperref[\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable}]{\sphinxcrossref{\sphinxcode{inferpy.models.random\_variable.RandomVariable}}}}
\index{PARAMS (inferpy.models.factory.InverseGamma attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.InverseGamma.PARAMS}}\pysigline{\sphinxbfcode{PARAMS}\sphinxbfcode{ = {[}'concentration', 'rate'{]}}}
\end{fulllineitems}

\index{\_\_init\_\_() (inferpy.models.factory.InverseGamma method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.InverseGamma.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{*args}, \emph{**kwargs}}{}
constructor for InverseGamma

\end{fulllineitems}

\index{concentration (inferpy.models.factory.InverseGamma attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.InverseGamma.concentration}}\pysigline{\sphinxbfcode{concentration}}
property for concentration

\end{fulllineitems}

\index{rate (inferpy.models.factory.InverseGamma attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.InverseGamma.rate}}\pysigline{\sphinxbfcode{rate}}
property for rate

\end{fulllineitems}


\end{fulllineitems}

\index{Laplace (class in inferpy.models.factory)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Laplace}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.factory.}\sphinxbfcode{Laplace}}{\emph{*args}, \emph{**kwargs}}{}
Bases: {\hyperref[\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable}]{\sphinxcrossref{\sphinxcode{inferpy.models.random\_variable.RandomVariable}}}}
\index{PARAMS (inferpy.models.factory.Laplace attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Laplace.PARAMS}}\pysigline{\sphinxbfcode{PARAMS}\sphinxbfcode{ = {[}'loc', 'scale'{]}}}
\end{fulllineitems}

\index{\_\_init\_\_() (inferpy.models.factory.Laplace method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Laplace.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{*args}, \emph{**kwargs}}{}
constructor for Laplace

\end{fulllineitems}

\index{loc (inferpy.models.factory.Laplace attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Laplace.loc}}\pysigline{\sphinxbfcode{loc}}
property for loc

\end{fulllineitems}

\index{scale (inferpy.models.factory.Laplace attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Laplace.scale}}\pysigline{\sphinxbfcode{scale}}
property for scale

\end{fulllineitems}


\end{fulllineitems}

\index{Multinomial (class in inferpy.models.factory)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Multinomial}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.factory.}\sphinxbfcode{Multinomial}}{\emph{*args}, \emph{**kwargs}}{}
Bases: {\hyperref[\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable}]{\sphinxcrossref{\sphinxcode{inferpy.models.random\_variable.RandomVariable}}}}
\index{PARAMS (inferpy.models.factory.Multinomial attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Multinomial.PARAMS}}\pysigline{\sphinxbfcode{PARAMS}\sphinxbfcode{ = {[}'total\_count', 'logits', 'probs'{]}}}
\end{fulllineitems}

\index{\_\_init\_\_() (inferpy.models.factory.Multinomial method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Multinomial.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{*args}, \emph{**kwargs}}{}
constructor for Multinomial

\end{fulllineitems}

\index{logits (inferpy.models.factory.Multinomial attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Multinomial.logits}}\pysigline{\sphinxbfcode{logits}}
property for logits

\end{fulllineitems}

\index{probs (inferpy.models.factory.Multinomial attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Multinomial.probs}}\pysigline{\sphinxbfcode{probs}}
property for probs

\end{fulllineitems}

\index{total\_count (inferpy.models.factory.Multinomial attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Multinomial.total_count}}\pysigline{\sphinxbfcode{total\_count}}
property for total\_count

\end{fulllineitems}


\end{fulllineitems}

\index{MultivariateNormalDiag (class in inferpy.models.factory)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.MultivariateNormalDiag}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.factory.}\sphinxbfcode{MultivariateNormalDiag}}{\emph{*args}, \emph{**kwargs}}{}
Bases: {\hyperref[\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable}]{\sphinxcrossref{\sphinxcode{inferpy.models.random\_variable.RandomVariable}}}}
\index{PARAMS (inferpy.models.factory.MultivariateNormalDiag attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.MultivariateNormalDiag.PARAMS}}\pysigline{\sphinxbfcode{PARAMS}\sphinxbfcode{ = {[}'loc', 'scale\_diag'{]}}}
\end{fulllineitems}

\index{\_\_init\_\_() (inferpy.models.factory.MultivariateNormalDiag method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.MultivariateNormalDiag.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{*args}, \emph{**kwargs}}{}
constructor for MultivariateNormalDiag

\end{fulllineitems}

\index{loc (inferpy.models.factory.MultivariateNormalDiag attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.MultivariateNormalDiag.loc}}\pysigline{\sphinxbfcode{loc}}
property for loc

\end{fulllineitems}

\index{scale\_diag (inferpy.models.factory.MultivariateNormalDiag attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.MultivariateNormalDiag.scale_diag}}\pysigline{\sphinxbfcode{scale\_diag}}
property for scale\_diag

\end{fulllineitems}


\end{fulllineitems}

\index{Normal (class in inferpy.models.factory)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Normal}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.factory.}\sphinxbfcode{Normal}}{\emph{*args}, \emph{**kwargs}}{}
Bases: {\hyperref[\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable}]{\sphinxcrossref{\sphinxcode{inferpy.models.random\_variable.RandomVariable}}}}
\index{PARAMS (inferpy.models.factory.Normal attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Normal.PARAMS}}\pysigline{\sphinxbfcode{PARAMS}\sphinxbfcode{ = {[}'loc', 'scale'{]}}}
\end{fulllineitems}

\index{\_\_init\_\_() (inferpy.models.factory.Normal method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Normal.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{*args}, \emph{**kwargs}}{}
constructor for Normal

\end{fulllineitems}

\index{loc (inferpy.models.factory.Normal attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Normal.loc}}\pysigline{\sphinxbfcode{loc}}
property for loc

\end{fulllineitems}

\index{scale (inferpy.models.factory.Normal attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Normal.scale}}\pysigline{\sphinxbfcode{scale}}
property for scale

\end{fulllineitems}


\end{fulllineitems}

\index{Poisson (class in inferpy.models.factory)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Poisson}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.factory.}\sphinxbfcode{Poisson}}{\emph{*args}, \emph{**kwargs}}{}
Bases: {\hyperref[\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable}]{\sphinxcrossref{\sphinxcode{inferpy.models.random\_variable.RandomVariable}}}}
\index{PARAMS (inferpy.models.factory.Poisson attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Poisson.PARAMS}}\pysigline{\sphinxbfcode{PARAMS}\sphinxbfcode{ = {[}{]}}}
\end{fulllineitems}

\index{\_\_init\_\_() (inferpy.models.factory.Poisson method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Poisson.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{*args}, \emph{**kwargs}}{}
constructor for Poisson

\end{fulllineitems}


\end{fulllineitems}

\index{Uniform (class in inferpy.models.factory)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Uniform}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.factory.}\sphinxbfcode{Uniform}}{\emph{*args}, \emph{**kwargs}}{}
Bases: {\hyperref[\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable}]{\sphinxcrossref{\sphinxcode{inferpy.models.random\_variable.RandomVariable}}}}
\index{PARAMS (inferpy.models.factory.Uniform attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Uniform.PARAMS}}\pysigline{\sphinxbfcode{PARAMS}\sphinxbfcode{ = {[}'low', 'high'{]}}}
\end{fulllineitems}

\index{\_\_init\_\_() (inferpy.models.factory.Uniform method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Uniform.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{*args}, \emph{**kwargs}}{}
constructor for Uniform

\end{fulllineitems}

\index{high (inferpy.models.factory.Uniform attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Uniform.high}}\pysigline{\sphinxbfcode{high}}
property for high

\end{fulllineitems}

\index{low (inferpy.models.factory.Uniform attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.Uniform.low}}\pysigline{\sphinxbfcode{low}}
property for low

\end{fulllineitems}


\end{fulllineitems}

\index{def\_random\_variable() (in module inferpy.models.factory)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.factory.def_random_variable}}\pysiglinewithargsret{\sphinxcode{inferpy.models.factory.}\sphinxbfcode{def\_random\_variable}}{\emph{var}}{}
\end{fulllineitems}



\subsubsection{inferpy.models.prob\_model module}
\label{\detokenize{modules/inferpy.models:inferpy-models-prob-model-module}}\label{\detokenize{modules/inferpy.models:module-inferpy.models.prob_model}}\index{inferpy.models.prob\_model (module)}
Module with the probabilistic model functionality.
\index{ProbModel (class in inferpy.models.prob\_model)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.prob\_model.}\sphinxbfcode{ProbModel}}{\emph{varlist=None}}{}
Bases: \sphinxcode{object}

Class implementing a probabilistic model

A probabilistic model defines a joint distribution over observed and latent variables. This
class encapsulates all the functionality for making inference (and learning) in these models.

An example of use:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{inferpy} \PYG{k}{as} \PYG{n+nn}{inf}
\PYG{k+kn}{from} \PYG{n+nn}{inferpy}\PYG{n+nn}{.}\PYG{n+nn}{models} \PYG{k}{import} \PYG{n}{Normal}



\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m}\PYG{p}{:}

    \PYG{n}{x} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{x}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} print the list of variables}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{m}\PYG{o}{.}\PYG{n}{varlist}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{m}\PYG{o}{.}\PYG{n}{latent\PYGZus{}vars}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{m}\PYG{o}{.}\PYG{n}{observed\PYGZus{}vars}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} get a sample}

\PYG{n}{m\PYGZus{}sample} \PYG{o}{=} \PYG{n}{m}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}



\PYG{c+c1}{\PYGZsh{} compute the log\PYGZus{}prob for each element in the sample}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{m}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{m\PYGZus{}sample}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} compute the sum of the log\PYGZus{}prob}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{m}\PYG{o}{.}\PYG{n}{sum\PYGZus{}log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{m\PYGZus{}sample}\PYG{p}{)}\PYG{p}{)}




\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} alternative definition}

\PYG{n}{x2} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{x2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{y2} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{m2} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{n}{varlist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{x2}\PYG{p}{,}\PYG{n}{y2}\PYG{p}{]}\PYG{p}{)}

\end{sphinxVerbatim}

This class can be used, for instance, for infering the parameters of some observed data:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{edward} \PYG{k}{as} \PYG{n+nn}{ed}
\PYG{k+kn}{import} \PYG{n+nn}{inferpy} \PYG{k}{as} \PYG{n+nn}{inf}


\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{} learning a 1\PYGZhy{}dim parameter from 1\PYGZhy{}dim data}


\PYG{n}{N} \PYG{o}{=} \PYG{l+m+mi}{50}
\PYG{n}{sampling\PYGZus{}mean} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mf}{30.}\PYG{p}{]}
\PYG{n}{sess} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{util}\PYG{o}{.}\PYG{n}{get\PYGZus{}session}\PYG{p}{(}\PYG{p}{)}


\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m}\PYG{p}{:}

    \PYG{n}{theta} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{)}

    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{N}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{theta}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{n}{m}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{x\PYGZus{}train} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{sampling\PYGZus{}mean}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{o}{.}\PYG{n}{name} \PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{\PYGZcb{}}


\PYG{n}{m}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}

\PYG{n}{m}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{)}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{}29.017122}


\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{} learning a 2 parameters of 1\PYGZhy{}dim from 2\PYGZhy{}dim data}

\PYG{n}{N} \PYG{o}{=} \PYG{l+m+mi}{50}
\PYG{n}{sampling\PYGZus{}mean} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mf}{30.}\PYG{p}{,} \PYG{l+m+mf}{10.}\PYG{p}{]}
\PYG{n}{sess} \PYG{o}{=} \PYG{n}{ed}\PYG{o}{.}\PYG{n}{util}\PYG{o}{.}\PYG{n}{get\PYGZus{}session}\PYG{p}{(}\PYG{p}{)}


\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{ProbModel}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{m}\PYG{p}{:}

    \PYG{n}{theta1} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{theta2} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}


    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{N}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{p}{[}\PYG{n}{theta1}\PYG{p}{,} \PYG{n}{theta2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}




\PYG{n}{m}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{p}{)}


\PYG{n}{x\PYGZus{}train} \PYG{o}{=} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{sampling\PYGZus{}mean}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{o}{.}\PYG{n}{name} \PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{\PYGZcb{}}


\PYG{n}{m}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}

\PYG{n}{m}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{loc}
\PYG{n}{m}\PYG{o}{.}\PYG{n}{posterior}\PYG{p}{(}\PYG{n}{theta2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{loc}

\end{sphinxVerbatim}
\index{\_\_init\_\_() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{varlist=None}}{}
Initializes a probabilistic model
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{varlist} \textendash{} optional list with the variables in the model

\end{description}\end{quote}

\end{fulllineitems}

\index{add\_var() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.add_var}}\pysiglinewithargsret{\sphinxbfcode{add\_var}}{\emph{v}}{}
Method for adding a new random variable. After use, the model should be re-compiled

\end{fulllineitems}

\index{compatible\_var() (inferpy.models.prob\_model.ProbModel static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.compatible_var}}\pysiglinewithargsret{\sphinxbfcode{static }\sphinxbfcode{compatible\_var}}{\emph{v}}{}
\end{fulllineitems}

\index{compile() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.compile}}\pysiglinewithargsret{\sphinxbfcode{compile}}{\emph{infMethod='KLqp'}, \emph{Q=None}, \emph{proposal\_vars=None}}{}
This method initializes the structures for making inference in the model.

\end{fulllineitems}

\index{copy() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.copy}}\pysiglinewithargsret{\sphinxbfcode{copy}}{\emph{swap\_dict=None}}{}
\end{fulllineitems}

\index{fit() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.fit}}\pysiglinewithargsret{\sphinxbfcode{fit}}{\emph{*args}, \emph{**kwargs}}{}
Assings data to the observed variables

\end{fulllineitems}

\index{get\_active\_model() (inferpy.models.prob\_model.ProbModel static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.get_active_model}}\pysiglinewithargsret{\sphinxbfcode{static }\sphinxbfcode{get\_active\_model}}{}{}
Return the active model defined with the construct ‘with’

\end{fulllineitems}

\index{get\_config() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.get_config}}\pysiglinewithargsret{\sphinxbfcode{get\_config}}{}{}
\end{fulllineitems}

\index{get\_copy\_from() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.get_copy_from}}\pysiglinewithargsret{\sphinxbfcode{get\_copy\_from}}{\emph{original\_var}}{}
\end{fulllineitems}

\index{get\_parents() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.get_parents}}\pysiglinewithargsret{\sphinxbfcode{get\_parents}}{\emph{v}}{}
\end{fulllineitems}

\index{get\_var() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.get_var}}\pysiglinewithargsret{\sphinxbfcode{get\_var}}{\emph{name}}{}
Get a varible in the model with a given name

\end{fulllineitems}

\index{get\_vardict() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.get_vardict}}\pysiglinewithargsret{\sphinxbfcode{get\_vardict}}{}{}
\end{fulllineitems}

\index{get\_vardict\_rev() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.get_vardict_rev}}\pysiglinewithargsret{\sphinxbfcode{get\_vardict\_rev}}{}{}
\end{fulllineitems}

\index{is\_active() (inferpy.models.prob\_model.ProbModel static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.is_active}}\pysiglinewithargsret{\sphinxbfcode{static }\sphinxbfcode{is\_active}}{}{}
Check if a replicate construct has been initialized
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
True if the method is inside a construct ProbModel (of size different to 1).
Otherwise False is return

\end{description}\end{quote}

\end{fulllineitems}

\index{is\_compiled() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.is_compiled}}\pysiglinewithargsret{\sphinxbfcode{is\_compiled}}{}{}
Determines if the model has been compiled

\end{fulllineitems}

\index{latent\_vars (inferpy.models.prob\_model.ProbModel attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.latent_vars}}\pysigline{\sphinxbfcode{latent\_vars}}
list of latent (i.e., non-observed) variables in the model

\end{fulllineitems}

\index{log\_prob() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.log_prob}}\pysiglinewithargsret{\sphinxbfcode{log\_prob}}{\emph{*args}, \emph{**kwargs}}{}
Computes the log probabilities of a (set of) sample(s)

\end{fulllineitems}

\index{no\_parents() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.no_parents}}\pysiglinewithargsret{\sphinxbfcode{no\_parents}}{}{}
\end{fulllineitems}

\index{observed\_vars (inferpy.models.prob\_model.ProbModel attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.observed_vars}}\pysigline{\sphinxbfcode{observed\_vars}}
list of observed variabels in the model

\end{fulllineitems}

\index{posterior() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.posterior}}\pysiglinewithargsret{\sphinxbfcode{posterior}}{\emph{*args}, \emph{**kwargs}}{}
Return the posterior distribution of some latent variables
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{latent\_var} \textendash{} a single or a set of latent variables in the model

\item[{Returns}] \leavevmode
Random variable(s) of the same type than the prior distributions

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.predict}}\pysiglinewithargsret{\sphinxbfcode{predict}}{\emph{target}, \emph{data}, \emph{reset\_tf\_vars=False}}{}
\end{fulllineitems}

\index{predict\_old() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.predict_old}}\pysiglinewithargsret{\sphinxbfcode{predict\_old}}{\emph{target\_var}, \emph{observations}}{}
\end{fulllineitems}

\index{reset\_compilation() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.reset_compilation}}\pysiglinewithargsret{\sphinxbfcode{reset\_compilation}}{}{}
Clear the structues created during the compilation of the model

\end{fulllineitems}

\index{sample() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.sample}}\pysiglinewithargsret{\sphinxbfcode{sample}}{\emph{*args}, \emph{**kwargs}}{}
Generates a sample for eache variable in the model

\end{fulllineitems}

\index{sum\_log\_prob() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.sum_log_prob}}\pysiglinewithargsret{\sphinxbfcode{sum\_log\_prob}}{\emph{*args}, \emph{**kwargs}}{}
Computes the sum of the log probabilities of a (set of) sample(s)

\end{fulllineitems}

\index{summary() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.summary}}\pysiglinewithargsret{\sphinxbfcode{summary}}{}{}
\end{fulllineitems}

\index{to\_json() (inferpy.models.prob\_model.ProbModel method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.to_json}}\pysiglinewithargsret{\sphinxbfcode{to\_json}}{}{}
\end{fulllineitems}

\index{varlist (inferpy.models.prob\_model.ProbModel attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.prob_model.ProbModel.varlist}}\pysigline{\sphinxbfcode{varlist}}
list of variables (observed and latent)

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{inferpy.models.random\_variable module}
\label{\detokenize{modules/inferpy.models:module-inferpy.models.random_variable}}\label{\detokenize{modules/inferpy.models:inferpy-models-random-variable-module}}\index{inferpy.models.random\_variable (module)}
Module implementing the shared functionality across all the variable types
\index{RandomVariable (class in inferpy.models.random\_variable)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.random\_variable.}\sphinxbfcode{RandomVariable}}{\emph{base\_object=None}, \emph{observed=False}}{}
Bases: \sphinxcode{object}

Base class for random variables.
\index{\_\_init\_\_() (inferpy.models.random\_variable.RandomVariable method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{base\_object=None}, \emph{observed=False}}{}
Constructor for the RandomVariable class
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{base\_object} \textendash{} encapsulated Edward object (optional).

\item {} 
\sphinxstyleliteralstrong{observed} (\sphinxstyleliteralemphasis{bool}) \textendash{} specifies if the random variable is observed (True) or observed (False).

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{base\_object (inferpy.models.random\_variable.RandomVariable attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.base_object}}\pysigline{\sphinxbfcode{base\_object}}
Underlying Tensorflow object

\end{fulllineitems}

\index{batches (inferpy.models.random\_variable.RandomVariable attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.batches}}\pysigline{\sphinxbfcode{batches}}
Number of batches of the variable

\end{fulllineitems}

\index{bind (inferpy.models.random\_variable.RandomVariable attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.bind}}\pysigline{\sphinxbfcode{bind}}
\end{fulllineitems}

\index{copy() (inferpy.models.random\_variable.RandomVariable method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.copy}}\pysiglinewithargsret{\sphinxbfcode{copy}}{\emph{swap\_dict=None}, \emph{observed=False}}{}
Build a new random variable with the same values.
The underlying tensors or edward objects are copied as well.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{swap\_dict} \textendash{} random variables, variables, tensors, or operations to swap with.

\item {} 
\sphinxstyleliteralstrong{observed} \textendash{} determines if the new variable is observed or not.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{dim (inferpy.models.random\_variable.RandomVariable attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.dim}}\pysigline{\sphinxbfcode{dim}}
Dimensionality of variable

\end{fulllineitems}

\index{dist (inferpy.models.random\_variable.RandomVariable attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.dist}}\pysigline{\sphinxbfcode{dist}}
Underlying Edward object

\end{fulllineitems}

\index{equal() (inferpy.models.random\_variable.RandomVariable method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.equal}}\pysiglinewithargsret{\sphinxbfcode{equal}}{\emph{other}}{}
documentation for equal

\end{fulllineitems}

\index{event\_shape (inferpy.models.random\_variable.RandomVariable attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.event_shape}}\pysigline{\sphinxbfcode{event\_shape}}
\end{fulllineitems}

\index{get\_key\_from\_var() (inferpy.models.random\_variable.RandomVariable static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.get_key_from_var}}\pysiglinewithargsret{\sphinxbfcode{static }\sphinxbfcode{get\_key\_from\_var}}{\emph{*args}, \emph{**kwargs}}{}
\end{fulllineitems}

\index{get\_local\_hidden() (inferpy.models.random\_variable.RandomVariable method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.get_local_hidden}}\pysiglinewithargsret{\sphinxbfcode{get\_local\_hidden}}{}{}
Returns a list with all the local hidden variables w.r.t. this one. Local hidden variables
are those latent variables which are in the same replicate construct.

\end{fulllineitems}

\index{get\_replicate\_list() (inferpy.models.random\_variable.RandomVariable method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.get_replicate_list}}\pysiglinewithargsret{\sphinxbfcode{get\_replicate\_list}}{}{}
Returns a list with all the replicate constructs that this variable belongs to.

\end{fulllineitems}

\index{get\_var\_with\_key() (inferpy.models.random\_variable.RandomVariable static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.get_var_with_key}}\pysiglinewithargsret{\sphinxbfcode{static }\sphinxbfcode{get\_var\_with\_key}}{\emph{*args}, \emph{**kwargs}}{}
\end{fulllineitems}

\index{is\_generic\_variable() (inferpy.models.random\_variable.RandomVariable method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.is_generic_variable}}\pysiglinewithargsret{\sphinxbfcode{is\_generic\_variable}}{}{}
Determines if this is a generic variable, i.e., an Edward variable
is not encapsulated.

\end{fulllineitems}

\index{log\_prob() (inferpy.models.random\_variable.RandomVariable method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.log_prob}}\pysiglinewithargsret{\sphinxbfcode{log\_prob}}{\emph{*args}, \emph{**kwargs}}{}
Method for computing the log probability of a sample v (or a set of samples)

\end{fulllineitems}

\index{mean() (inferpy.models.random\_variable.RandomVariable method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.mean}}\pysiglinewithargsret{\sphinxbfcode{mean}}{\emph{*args}, \emph{**kwargs}}{}
Method for obaining the mean of this random variable

\end{fulllineitems}

\index{name (inferpy.models.random\_variable.RandomVariable attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.name}}\pysigline{\sphinxbfcode{name}}
name of the variable

\end{fulllineitems}

\index{observed (inferpy.models.random\_variable.RandomVariable attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.observed}}\pysigline{\sphinxbfcode{observed}}
boolean property that determines if a variable is observed or not

\end{fulllineitems}

\index{prob() (inferpy.models.random\_variable.RandomVariable method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.prob}}\pysiglinewithargsret{\sphinxbfcode{prob}}{\emph{*args}, \emph{**kwargs}}{}
Method for computing the probability of a sample v (or a set of samples)

\end{fulllineitems}

\index{prod\_prob() (inferpy.models.random\_variable.RandomVariable method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.prod_prob}}\pysiglinewithargsret{\sphinxbfcode{prod\_prob}}{\emph{*args}, \emph{**kwargs}}{}
Method for computing the joint probability of a sample v (or a set of samples)

\end{fulllineitems}

\index{sample() (inferpy.models.random\_variable.RandomVariable method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.sample}}\pysiglinewithargsret{\sphinxbfcode{sample}}{\emph{*args}, \emph{**kwargs}}{}
Method for obaining a samples
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{size} \textendash{} scalar or matrix of integers indicating the shape of the matrix of samples.

\item[{Returns}] \leavevmode
Matrix of samples. Each element in the output matrix has the same shape than the variable.

\end{description}\end{quote}

\end{fulllineitems}

\index{shape (inferpy.models.random\_variable.RandomVariable attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.shape}}\pysigline{\sphinxbfcode{shape}}
shape of the variable, i.e. (batches, dim)

\end{fulllineitems}

\index{stddev() (inferpy.models.random\_variable.RandomVariable method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.stddev}}\pysiglinewithargsret{\sphinxbfcode{stddev}}{\emph{*args}, \emph{**kwargs}}{}
Method for obataining the standard deviation of this random variable

\end{fulllineitems}

\index{sum\_log\_prob() (inferpy.models.random\_variable.RandomVariable method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.sum_log_prob}}\pysiglinewithargsret{\sphinxbfcode{sum\_log\_prob}}{\emph{*args}, \emph{**kwargs}}{}
Method for computing the sum of the log probability of a sample v (or a set of samples)

\end{fulllineitems}

\index{variance() (inferpy.models.random\_variable.RandomVariable method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.random_variable.RandomVariable.variance}}\pysiglinewithargsret{\sphinxbfcode{variance}}{\emph{*args}, \emph{**kwargs}}{}
Method for obataining the variance of this random variable

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{inferpy.models.replicate module}
\label{\detokenize{modules/inferpy.models:inferpy-models-replicate-module}}\label{\detokenize{modules/inferpy.models:module-inferpy.models.replicate}}\index{inferpy.models.replicate (module)}
Module with the replication functionality.
\index{replicate (class in inferpy.models.replicate)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.replicate.replicate}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{inferpy.models.replicate.}\sphinxbfcode{replicate}}{\emph{size}}{}
Class implementing the Plateau notation

The plateau notation is used to replicate the random variables contained
within this construct. Every replicated variable is conditionally idependent
given the previous random variables (if any) defined outside the with statement.
The \sphinxcode{with inf.replicate(size = N)} sintaxis is used to replicate N times the
contained definitions. For example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{inferpy} \PYG{k}{as} \PYG{n+nn}{inf}


\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Define some random variables here}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variable replicated }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{o}{.}\PYG{n}{get\PYGZus{}total\PYGZus{}size}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ times}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}


\PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Define some random variables here}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variable replicated }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{o}{.}\PYG{n}{get\PYGZus{}total\PYGZus{}size}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ times}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{with} \PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} Define some random variables here}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variable replicated }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{o}{.}\PYG{n}{get\PYGZus{}total\PYGZus{}size}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ times}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{o}{.}\PYG{n}{in\PYGZus{}replicate}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Define some random variables here}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variable replicated }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{replicate}\PYG{o}{.}\PYG{n}{get\PYGZus{}total\PYGZus{}size}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ times}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}



\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} compound replicates}

\PYG{c+c1}{\PYGZsh{} set name property}
\PYG{c+c1}{\PYGZsh{} new inputs: name, compound}

\PYG{c+c1}{\PYGZsh{} default name: should not exists}

\PYG{c+c1}{\PYGZsh{} if already name exists, the size can be omitted or should be consistent}
\PYG{c+c1}{\PYGZsh{}}


\PYG{c+c1}{\PYGZsh{} como activar}
\end{sphinxVerbatim}

The number of times that indicated with input argument \sphinxcode{size}.
Note that nested replicate constructs can be defined as well. At any moment,
the product of all the nested replicate constructs can be obtained by
invoking the static method \sphinxcode{get\_total\_size()}.

\begin{sphinxadmonition}{note}{Note:}
Defining a variable inside the construct replicate with size equal to 1, that is,
\sphinxcode{inf.replicate(size=1)} is equivalent to defining outside any replicate
construct.
\end{sphinxadmonition}
\index{\_\_init\_\_() (inferpy.models.replicate.replicate method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.replicate.replicate.__init__}}\pysiglinewithargsret{\sphinxbfcode{\_\_init\_\_}}{\emph{size}}{}
Initializes the replicate construct
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{size} (\sphinxstyleliteralemphasis{int}) \textendash{} number of times that the variables contained are replicated.

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_active\_replicate() (inferpy.models.replicate.replicate static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.replicate.replicate.get_active_replicate}}\pysiglinewithargsret{\sphinxbfcode{static }\sphinxbfcode{get\_active\_replicate}}{}{}
Return the active replicate defined with the construct ‘with’

\end{fulllineitems}

\index{get\_all\_replicate() (inferpy.models.replicate.replicate static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.replicate.replicate.get_all_replicate}}\pysiglinewithargsret{\sphinxbfcode{static }\sphinxbfcode{get\_all\_replicate}}{}{}
\end{fulllineitems}

\index{get\_total\_size() (inferpy.models.replicate.replicate static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.replicate.replicate.get_total_size}}\pysiglinewithargsret{\sphinxbfcode{static }\sphinxbfcode{get\_total\_size}}{}{}
Static method that returns the product of the sizes of all the nested replicate constructs
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
Integer with the product of sizes

\end{description}\end{quote}

\end{fulllineitems}

\index{in\_replicate() (inferpy.models.replicate.replicate static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.replicate.replicate.in_replicate}}\pysiglinewithargsret{\sphinxbfcode{static }\sphinxbfcode{in\_replicate}}{}{}
Check if a replicate construct has been initialized
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
True if the method is inside a construct replicate (of size different to 1).
Otherwise False is return

\end{description}\end{quote}

\end{fulllineitems}

\index{print\_total\_size() (inferpy.models.replicate.replicate static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.models:inferpy.models.replicate.replicate.print_total_size}}\pysiglinewithargsret{\sphinxbfcode{static }\sphinxbfcode{print\_total\_size}}{}{}
Static that prints the total size

\end{fulllineitems}


\end{fulllineitems}



\subsection{inferpy.util package}
\label{\detokenize{modules/inferpy.util:inferpy-util-package}}\label{\detokenize{modules/inferpy.util::doc}}

\subsubsection{Submodules}
\label{\detokenize{modules/inferpy.util:submodules}}

\subsubsection{inferpy.util.error module}
\label{\detokenize{modules/inferpy.util:module-inferpy.util.error}}\label{\detokenize{modules/inferpy.util:inferpy-util-error-module}}\index{inferpy.util.error (module)}
Module implementing custom exceptions and errors
\index{ScopeException}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.util:inferpy.util.error.ScopeException}}\pysigline{\sphinxbfcode{exception }\sphinxcode{inferpy.util.error.}\sphinxbfcode{ScopeException}}
Bases: \sphinxcode{exceptions.Exception}

This exception is raised when an InferPy object is declared in a non-valid scope

\end{fulllineitems}



\subsubsection{inferpy.util.format module}
\label{\detokenize{modules/inferpy.util:inferpy-util-format-module}}\label{\detokenize{modules/inferpy.util:module-inferpy.util.format}}\index{inferpy.util.format (module)}
Module implementing text formatting operaionts
\index{np\_str() (in module inferpy.util.format)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.util:inferpy.util.format.np_str}}\pysiglinewithargsret{\sphinxcode{inferpy.util.format.}\sphinxbfcode{np\_str}}{\emph{s}}{}
Shorten string representation of a numpy object
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{s} \textendash{} numpy object.

\end{description}\end{quote}

\end{fulllineitems}



\subsubsection{inferpy.util.ops module}
\label{\detokenize{modules/inferpy.util:inferpy-util-ops-module}}\label{\detokenize{modules/inferpy.util:module-inferpy.util.ops}}\index{inferpy.util.ops (module)}
Module implementing some useful operations over tensors and random variables
\index{case() (in module inferpy.util.ops)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.util:inferpy.util.ops.case}}\pysiglinewithargsret{\sphinxcode{inferpy.util.ops.}\sphinxbfcode{case}}{\emph{d}, \emph{default=None}, \emph{exclusive=True}, \emph{strict=False}, \emph{name='case'}}{}
Control flow operation depending of the outcome of a tensor. Any expression
in tensorflow giving as a result a boolean is allowed as condition.

Internally, the operation tensorflow.case is invoked. Unlike the tensorflow operation, this one
accepts InferPy variables as input parameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{d} \textendash{} dictionary where the keys are the conditions (i.e. boolean tensor).

\item {} 
\sphinxstyleliteralstrong{exclusive} \textendash{} True iff at most one case is allowed to evaluate to True.

\item {} 
\sphinxstyleliteralstrong{name} \textendash{} name of the resulting tensor.

\end{itemize}

\item[{Returns}] \leavevmode
Tensor implementing the case operation. This is the output of the operation
tensorflow.case internally invoked.

\end{description}\end{quote}

\end{fulllineitems}

\index{case\_states() (in module inferpy.util.ops)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.util:inferpy.util.ops.case_states}}\pysiglinewithargsret{\sphinxcode{inferpy.util.ops.}\sphinxbfcode{case\_states}}{\emph{var}, \emph{d}, \emph{default=None}, \emph{exclusive=True}, \emph{strict=False}, \emph{name='case'}}{}
Control flow operation depending of the outcome of a discrete variable.

Internally, the operation tensorflow.case is invoked. Unlike the tensorflow operation, this one
accepts InferPy variables as input parameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{var} \textendash{} Control InferPy discrete random variable.

\item {} 
\sphinxstyleliteralstrong{d} \textendash{} dictionary where the keys are each of the possible values of control variable

\item {} 
\sphinxstyleliteralstrong{the values are returning tensors for each case.} (\sphinxstyleliteralemphasis{and}) \textendash{} 

\item {} 
\sphinxstyleliteralstrong{exclusive} \textendash{} True iff at most one case is allowed to evaluate to True.

\item {} 
\sphinxstyleliteralstrong{name} \textendash{} name of the resulting tensor.

\end{itemize}

\item[{Returns}] \leavevmode
Tensor implementing the case operation. This is the output of the operation
tensorflow.case internally invoked.

\end{description}\end{quote}

\end{fulllineitems}

\index{dot() (in module inferpy.util.ops)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.util:inferpy.util.ops.dot}}\pysiglinewithargsret{\sphinxcode{inferpy.util.ops.}\sphinxbfcode{dot}}{\emph{x}, \emph{y}}{}
Compute dot product between an InferPy or Tensor object. The number of batches N equal to 1
for one of them, and higher for the other one.
\begin{quote}

If necessarily, the order of the operands may be changed.
\begin{description}
\item[{Args:}] \leavevmode
x: first operand. This could be an InferPy variable, a Tensor, a numpy object or a numeric Python list.
x: second operand. This could be an InferPy variable, a Tensor, a numpy object or a numeric Python list.

\end{description}
\end{quote}
\begin{description}
\item[{Retruns:}] \leavevmode
An InferPy variable of type Deterministic encapsulating the resulting tensor
of the multiplications.

\end{description}

\end{fulllineitems}

\index{fix\_shape() (in module inferpy.util.ops)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.util:inferpy.util.ops.fix_shape}}\pysiglinewithargsret{\sphinxcode{inferpy.util.ops.}\sphinxbfcode{fix\_shape}}{\emph{s}}{}
Transforms a shape list into a standard InferPy shape format.

\end{fulllineitems}

\index{gather() (in module inferpy.util.ops)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.util:inferpy.util.ops.gather}}\pysiglinewithargsret{\sphinxcode{inferpy.util.ops.}\sphinxbfcode{gather}}{\emph{params}, \emph{indices}, \emph{validate\_indices=None}, \emph{name=None}, \emph{axis=0}}{}
Operation for selecting some of the items in a tensor.

Internally, the operation tensorflow.gather is invoked. Unlike the tensorflow operation, this one
accepts InferPy variables as input parameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{params} \textendash{} A Tensor. The tensor from which to gather values. Must be at least rank axis + 1.

\item {} 
\sphinxstyleliteralstrong{indices} \textendash{} A Tensor. Must be one of the following types: int32, int64. Index tensor. Must be in range

\item {} 
\sphinxstyleliteralstrong{params.shape}\sphinxstyleliteralstrong{{[}}\sphinxstyleliteralstrong{axis}\sphinxstyleliteralstrong{{]}}\sphinxstyleliteralstrong{)}\sphinxstyleliteralstrong{} (\sphinxstyleliteralemphasis{{[}}\sphinxstyleliteralemphasis{0}\sphinxstyleliteralemphasis{,}) \textendash{} 

\item {} 
\sphinxstyleliteralstrong{axis} \textendash{} A Tensor. Must be one of the following types: int32, int64. The axis in params to gather indices

\item {} 
\sphinxstyleliteralstrong{Defaults to the first dimension. Supports negative indexes.} (\sphinxstyleliteralemphasis{from.}) \textendash{} 

\item {} 
\sphinxstyleliteralstrong{name} \textendash{} A name for the operation (optional).

\end{itemize}

\item[{Returns}] \leavevmode
A Tensor. Has the same type as params.. This is the output of the operation
tensorflow.gather internally invoked.

\end{description}\end{quote}

\end{fulllineitems}

\index{matmul() (in module inferpy.util.ops)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.util:inferpy.util.ops.matmul}}\pysiglinewithargsret{\sphinxcode{inferpy.util.ops.}\sphinxbfcode{matmul}}{\emph{a}, \emph{b}, \emph{transpose\_a=False}, \emph{transpose\_b=False}, \emph{adjoint\_a=False}, \emph{adjoint\_b=False}, \emph{a\_is\_sparse=False}, \emph{b\_is\_sparse=False}, \emph{name=None}}{}
Matrix multiplication.

Input objects may be tensors but also InferPy variables.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{a} \textendash{} Tensor of type float16, float32, float64, int32, complex64, complex128 and rank \textgreater{} 1.

\item {} 
\sphinxstyleliteralstrong{b} \textendash{} Tensor with same type and rank as a.

\item {} 
\sphinxstyleliteralstrong{transpose\_a} \textendash{} If True, a is transposed before multiplication.

\item {} 
\sphinxstyleliteralstrong{transpose\_b} \textendash{} If True, b is transposed before multiplication.

\item {} 
\sphinxstyleliteralstrong{adjoint\_a} \textendash{} If True, a is conjugated and transposed before multiplication.

\item {} 
\sphinxstyleliteralstrong{adjoint\_b} \textendash{} If True, b is conjugated and transposed before multiplication.

\item {} 
\sphinxstyleliteralstrong{a\_is\_sparse} \textendash{} If True, a is treated as a sparse matrix.

\item {} 
\sphinxstyleliteralstrong{b\_is\_sparse} \textendash{} If True, b is treated as a sparse matrix.

\item {} 
\sphinxstyleliteralstrong{name} \textendash{} Name for the operation (optional).

\end{itemize}

\end{description}\end{quote}
\begin{description}
\item[{Retruns:}] \leavevmode
An InferPy variable of type Deterministic encapsulating the resulting tensor
of the multiplications.

\end{description}

\end{fulllineitems}

\index{param\_to\_tf() (in module inferpy.util.ops)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.util:inferpy.util.ops.param_to_tf}}\pysiglinewithargsret{\sphinxcode{inferpy.util.ops.}\sphinxbfcode{param\_to\_tf}}{\emph{x}}{}
Transforms either a scalar or a random variable into a Tensor

\end{fulllineitems}

\index{shape\_to\_list() (in module inferpy.util.ops)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.util:inferpy.util.ops.shape_to_list}}\pysiglinewithargsret{\sphinxcode{inferpy.util.ops.}\sphinxbfcode{shape\_to\_list}}{\emph{a}}{}
Transforms the shape of an object into a list
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{a} \textendash{} object whose shape will be transformed. This could be an InferPy variable, a Tensor, a numpy object or a numeric Python list.

\end{description}\end{quote}

\end{fulllineitems}



\subsubsection{inferpy.util.runtime module}
\label{\detokenize{modules/inferpy.util:module-inferpy.util.runtime}}\label{\detokenize{modules/inferpy.util:inferpy-util-runtime-module}}\index{inferpy.util.runtime (module)}
Module with useful definitions to be used in runtime
\index{get\_session() (in module inferpy.util.runtime)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.util:inferpy.util.runtime.get_session}}\pysiglinewithargsret{\sphinxcode{inferpy.util.runtime.}\sphinxbfcode{get\_session}}{}{}
Get the default tensorflow session

\end{fulllineitems}



\subsubsection{inferpy.util.wrappers module}
\label{\detokenize{modules/inferpy.util:inferpy-util-wrappers-module}}\label{\detokenize{modules/inferpy.util:module-inferpy.util.wrappers}}\index{inferpy.util.wrappers (module)}
Module with useful wrappers used for the development of InferPy.
\index{input\_model\_data() (in module inferpy.util.wrappers)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.util:inferpy.util.wrappers.input_model_data}}\pysiglinewithargsret{\sphinxcode{inferpy.util.wrappers.}\sphinxbfcode{input\_model\_data}}{\emph{f}}{}
wrapper that transforms, if required, a dataset object, making it suitable for InferPy inference
process.

\end{fulllineitems}

\index{multishape() (in module inferpy.util.wrappers)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.util:inferpy.util.wrappers.multishape}}\pysiglinewithargsret{\sphinxcode{inferpy.util.wrappers.}\sphinxbfcode{multishape}}{\emph{f}}{}
This wrapper allows to apply a function with simple parameters, over multidimensional ones.

\end{fulllineitems}

\index{singleton() (in module inferpy.util.wrappers)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.util:inferpy.util.wrappers.singleton}}\pysiglinewithargsret{\sphinxcode{inferpy.util.wrappers.}\sphinxbfcode{singleton}}{\emph{class\_}}{}
wrapper that allows to define a singleton class

\end{fulllineitems}

\index{static\_multishape() (in module inferpy.util.wrappers)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.util:inferpy.util.wrappers.static_multishape}}\pysiglinewithargsret{\sphinxcode{inferpy.util.wrappers.}\sphinxbfcode{static\_multishape}}{\emph{f}}{}
This wrapper allows to apply a function with simple parameters, over multidimensional ones.

\end{fulllineitems}

\index{tf\_run\_wrapper() (in module inferpy.util.wrappers)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/inferpy.util:inferpy.util.wrappers.tf_run_wrapper}}\pysiglinewithargsret{\sphinxcode{inferpy.util.wrappers.}\sphinxbfcode{tf\_run\_wrapper}}{\emph{f}}{}
When setted to a function f, this wrappers replaces the output tensor of f by its evaluation
in the default tensorflow session. In doing so, the API user will only work with standard Python
types.

\end{fulllineitems}



\section{Submodules}
\label{\detokenize{modules/inferpy:submodules}}

\section{inferpy.version module}
\label{\detokenize{modules/inferpy:module-inferpy.version}}\label{\detokenize{modules/inferpy:inferpy-version-module}}\index{inferpy.version (module)}
Version of inferpy

This module contains a constant string with the version of inferpy, i.e.:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{inferpy} \PYG{k}{as} \PYG{n+nn}{inf}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{inf}\PYG{o}{.}\PYG{n}{VERSION}\PYG{p}{)}
\end{sphinxVerbatim}


\chapter{Contact and Support}
\label{\detokenize{notes/contact:contact-and-support}}\label{\detokenize{notes/contact::doc}}
If you have any question about the toolbox or if you want to collaborate in the project, please do not hesitate to
contact us. You can do it through the following email address: \sphinxhref{mailto:inferpy.api@gmail.com}{inferpy.api@gmail.com}

For more technical questions, please use \sphinxhref{https://github.com/PGM-Lab/InferPy/issues}{Github issues}.


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{c}
\item {\sphinxstyleindexentry{inferpy.criticism.evaluate}}\sphinxstyleindexpageref{modules/inferpy.criticism:\detokenize{module-inferpy.criticism.evaluate}}
\indexspace
\bigletter{i}
\item {\sphinxstyleindexentry{inferpy.inferences.inference}}\sphinxstyleindexpageref{modules/inferpy.inferences:\detokenize{module-inferpy.inferences.inference}}
\item {\sphinxstyleindexentry{inferpy.inferences.qmodel}}\sphinxstyleindexpageref{modules/inferpy.inferences:\detokenize{module-inferpy.inferences.qmodel}}
\indexspace
\bigletter{m}
\item {\sphinxstyleindexentry{inferpy.models.deterministic}}\sphinxstyleindexpageref{modules/inferpy.models:\detokenize{module-inferpy.models.deterministic}}
\item {\sphinxstyleindexentry{inferpy.models.factory}}\sphinxstyleindexpageref{modules/inferpy.models:\detokenize{module-inferpy.models.factory}}
\item {\sphinxstyleindexentry{inferpy.models.prob\_model}}\sphinxstyleindexpageref{modules/inferpy.models:\detokenize{module-inferpy.models.prob_model}}
\item {\sphinxstyleindexentry{inferpy.models.random\_variable}}\sphinxstyleindexpageref{modules/inferpy.models:\detokenize{module-inferpy.models.random_variable}}
\item {\sphinxstyleindexentry{inferpy.models.replicate}}\sphinxstyleindexpageref{modules/inferpy.models:\detokenize{module-inferpy.models.replicate}}
\indexspace
\bigletter{u}
\item {\sphinxstyleindexentry{inferpy.util.error}}\sphinxstyleindexpageref{modules/inferpy.util:\detokenize{module-inferpy.util.error}}
\item {\sphinxstyleindexentry{inferpy.util.format}}\sphinxstyleindexpageref{modules/inferpy.util:\detokenize{module-inferpy.util.format}}
\item {\sphinxstyleindexentry{inferpy.util.ops}}\sphinxstyleindexpageref{modules/inferpy.util:\detokenize{module-inferpy.util.ops}}
\item {\sphinxstyleindexentry{inferpy.util.runtime}}\sphinxstyleindexpageref{modules/inferpy.util:\detokenize{module-inferpy.util.runtime}}
\item {\sphinxstyleindexentry{inferpy.util.wrappers}}\sphinxstyleindexpageref{modules/inferpy.util:\detokenize{module-inferpy.util.wrappers}}
\indexspace
\bigletter{v}
\item {\sphinxstyleindexentry{inferpy.version}}\sphinxstyleindexpageref{modules/inferpy:\detokenize{module-inferpy.version}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}