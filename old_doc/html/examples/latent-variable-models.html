<h1 id="sec:lvmodels">Tutorial: Easy Machine Learning with Latent Variable Models in AMIDST</h1>
<p>In AMIDST toolbox 0.4.2 the module <em>latent-variable-models</em>, that contains a wide range of predefined latent variable models (see table below), was included. In this tutorial we will show how the use of this module simplifies the learning and inference processes. In fact, you will be able to learn your model and to perform inference with a few lines of code.</p>
<div class="figure" style="text-align:center">
<img src="http://www.amidsttoolbox.com/wp-content/uploads/2018/04/amidstModels.png" alt="Set of predefined latent variable models in AMIDST" width="491" /><p class="caption" style="text-align:center">Set of predefined latent variable models in AMIDST<span data-label="fig:lvmodels:amidstModels"></span></p>
</div>
<p>Besides of the simplicity, the required code for learning a latent variable model is also flexible: you will be able to change the learnt model or the inference algorithm just with some slight modfications in the code. Another advantage of using AMIDST for learning one of the predefined models is that the procedure is traparent to the format of your training data: you will use the same methods regardless of learning from a local or a distributed dataset (with Flink). Note that this last feature was included in the version 0.5.0 of the toolbox.</p>
<h2 id="setting-up">Setting up</h2>
<p>In order to follow this tutorial, you will need to have the java 8 (i.e. SDK 1.8 or higher) installed in your system. For more details about the system requirements, see this <a href="../GettingStarted/requirements/">link</a>. Additionally, you can download a ready-to-use IntelliJ maven project with all the code examples in this tutorial. For that, use the following command:</p><pre><code>$ git clone https://github.com/amidst/tutorial.git      </code></pre>
<p>Alternativelly, you would rather create a new maven project or use an existing one. For that, you might check the <a href="../GettingStarted/index/">Getting Started</a> page.</p><p>Note that this project does not contain any AMIDST source or binary, it only has some .java files using the AMIDST functionallity. Instead, each of the AMIDST modules are provided through maven. Doing that, the transitive dependences of the AMIDST toolbox are also downloaded in a transparent way for the user. An scheme of this idea is shown below:</p>
<div class="figure" style="text-align:center">
<img src="http://www.amidsttoolbox.com/wp-content/uploads/2018/04/overview.png" alt="Set of predefined latent variable models in AMIDST" width="491" /><p class="caption" style="text-align:center">Set of predefined latent variable models in AMIDST<span data-label="fig:lvmodels:overview"></span></p>
</div>
<p>If we open the downloaded project, we will see that it contains the following relevant folders and files:</p>
<ul>
<li><p><strong>datasets</strong>: folder with local and distributed datasets used in this tutorial in ARFF format.</p></li>
<li><p><strong>doc</strong>: folder containing documentation about this tutorial.</p></li>
<li><p><strong>lib</strong>: folder for storing those libraries not available through maven.</p></li>
<li><p><strong>src/main/java</strong>: folder with all the code example.</p></li>
<li><p><strong>pom.xml</strong>: this is the maven configuration file where the AMIDST dependencies are defined.</p></li>
</ul>
<p>In the pom.xml file of the downloaded project, only the module called <em>latent-variable-models</em> is linked. However some other AMIDST are also loaded as <em>latent-variable-models</em> module depends on them. This is the case of the modules called <em>core</em>, <em>core-dynamic</em>, <em>flinklink</em>, etc. You can see the full list of dependencies in the <strong>maven project panel</strong>, usually located on the right side of the window (see image below). If dependencies are not automatically downloaded, click on <strong>Upload</strong> button. It is recommended to download the sources and java</p>
<div class="figure" style="text-align:center">
<img src="http://www.amidsttoolbox.com/wp-content/uploads/2018/04/mavenpanel.png" alt="Maven project panel showing the list of dependencies" width="377" /><p class="caption" style="text-align:center">Maven project panel showing the list of dependencies<span data-label="fig:lvmodels:mavenpanel"></span></p>
</div>
<h2 id="sec:lvmodels:static">Static Models</h2>
<h3 id="sec:lvmodels:static:learning">Learning and saving to disk</h3>
<p>Here we will see how can we learnt a static model from a local dataset (non-distributed). In particular, we will use the financial-like dataset <strong>datasets/simulated/cajamar.arff</strong> containing 4 continuous (normal distributed) variables. From this data, a <em>Factor Analysis</em> model will be learnt. In short, this model aims to reduce the dimensionality of a set of observed continuous variables by expressing them as combination of gaussians. A synthesis of this process is shown in the image below where: <span class="math inline"><em>X</em>1, <em>X</em>2, <em>X</em>3</span> and <span class="math inline"><em>X</em>4</span> are the observed variables and <span class="math inline"><em>H</em>1, <em>H</em>2</span> and <span class="math inline"><em>H</em>3</span> are the latent variables representing the combination of gaussians.</p>
<div class="figure" style="text-align:center">
<img src="http://www.amidsttoolbox.com/wp-content/uploads/2018/04/staticlearning.png" alt="Static learning process" width="377" /><p class="caption" style="text-align:center">Static learning process<span data-label="fig:lvmodels:static:learning:scheme"></span></p>
</div>
<p>Note that the required functionality for learning the predefined model is provided by the module <em>latent-variable-models</em>. A code-example for learning a factor analysis model is shown below.</p>
<pre><code>public class StaticModelLearning {
    public static void main(String[] args) throws ExceptionHugin, IOException {

        //Load the datastream
        String filename = &quot;datasets/simulated/cajamar.arff&quot;;
        DataStream&lt;DataInstance&gt; data = DataStreamLoader.open(filename);

        //Learn the model
        Model model = new FactorAnalysis(data.getAttributes());
       // ((MixtureOfFactorAnalysers)model).setNumberOfLatentVariables(3);

        model.updateModel(data);
        BayesianNetwork bn = model.getModel();

        System.out.println(bn);

        // Save with .bn format
        BayesianNetworkWriter.save(bn, &quot;networks/simulated/exampleBN.bn&quot;);

        // Save with hugin format
        //BayesianNetworkWriterToHugin.save(bn, &quot;networks/simulated/exampleBN.net&quot;);
    }

}</code></pre>
<p><a href="https://github.com/amidst/tutorial/blob/master/src/main/java/eu/amidst/tutorial/usingAmidst/examples/StaticModelLearning.java">See on GitHub</a></p>
<p>For learning any of the available static models, we create an object of any of the classes inheriting from the class <em>Model</em>. These classes encapsulates all the fuctionality for learning/updating a latent-variable model. For example, in the code above we create an object of the class <em>FactorAnalysis</em> which is actually stored as <em>Model</em> object. The flexibility of the toolbox is due to this hierarchical desing: if we aim to change the model learnt from our data, we simply have to change the constructor used (assuming that our data also fits the constraints of the new model). For example, if we aim to learn a mixture of factor analysers instead, we simply have to replace the line</p>
<pre><code>Model model = new FactorAnalysis(data.getAtributes());</code></pre>
<p>by</p>
<pre><code>Model model = new MixtureOfFactorAnalysers(data.getAtributes());</code></pre>
<p>Note that the method for learning the model, namely <em>Model::updateMode(DataStream&lt;DataInstance&gt;)</em> will always be the same regardless of the particular type of static model.</p>
<p>The actual learnt model is an object of the class <em>BayesianNetwork</em> which is stored as a member variable of <em>Model</em>. Thus, for using the network, we simply have to extract with the method <em>Model::getModel()</em>. One of the actions we can perform with it is saving it into the local file system. For saving it in <em>.bn</em> format:</p>
<pre><code>BayesianNetworkWriter::save(BayesianNetwork bn, String path)</code></pre>
<p>Alternatively, and assuming that we have the hugin library available, we can also save it in <em>.net</em> format:</p>
<pre><code>BayesianNetworkWriteToHuginr::save(BayesianNetwork bn, String path)</code></pre>
<h3 id="sec:lvmodels:static:flinklearning">Learning from Flink</h3>
<p>In previous section we showed how the AMIDST toolbox can be used for learning a static model from a non-distributed dataset. In addition, you can use the pre-defined models to process massive data sets in a distributed computer cluster using <strong>Apache Flink</strong>. In particular, the model can be learnt from a <em>distributed ARFF folder</em> or from a file accesible via a HDFS url. A scheme of the learning process is shown below.</p>
<div class="figure" style="text-align:center">
<img src="http://www.amidsttoolbox.com/wp-content/uploads/2018/04/distributedlearning.png" alt="Distributed learning process scheme" width="377" /><p class="caption" style="text-align:center">Distributed learning process scheme<span data-label="fig:lvmodels:static:flinklearning:scheme"></span></p>
</div>
<p>A code example for learning from Flink is shown below. Note that it only differs from the one in previous section in lines 25 to 33. In these lines, the Flink session is configurated and the stream is loaded, which is managed with an object of the class <em>DataFlink</em> (instead of <em>DataStream</em>).</p>
<pre><code>public class StaticModelFlink {
    public static void main(String[] args) throws IOException, ExceptionHugin {


        //Set-up Flink session.
       // Configuration conf = new Configuration();
      //  conf.setInteger(&quot;taskmanager.network.numberOfBuffers&quot;, 12000);
        final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
        //env.getConfig().disableSysoutLogging();
       // env.setParallelism(Main.PARALLELISM);

        //Load the datastream
        String filename = &quot;datasets/simulated/cajamarDistributed.arff&quot;;
        DataFlink&lt;DataInstance&gt; data = DataFlinkLoader.open(env, filename, false);

        //Learn the model
        Model model = new FactorAnalysis(data.getAttributes());
        model.updateModel(data);
        BayesianNetwork bn = model.getModel();

        System.out.println(bn);

        // Save with .bn format
        BayesianNetworkWriter.save(bn, &quot;networks/simulated/exampleBN.bn&quot;);

        // Save with hugin format
        //BayesianNetworkWriterToHugin.save(bn, &quot;networks/simulated/exampleBN.net&quot;);

    }

}</code></pre>
<p><a href="https://github.com/amidst/tutorial/blob/master/src/main/java/eu/amidst/tutorial/usingAmidst/examples/StaticModelFlink.java">See on GitHub</a></p>
<p>In previous example, the distributed dataset is stored in our local file system. Instead, we might need to load from a distributed file system. For that, simply replace the string indicating the path. That is, replace</p>
<pre><code>String filename = &quot;datasets/simulated/cajamarDistributed.arff&quot;</code></pre>
<p>by</p>
<pre><code>String filename = &quot;hdfs://distributed-server/path-to-file&quot;</code></pre>
<h3 id="sec:lvmodels:static:inference">Inference</h3>
<p>Making probabilistic inference in BNs (a.k.a <em>belief updating</em>) consists of the computation of the posterior probability distribution for a set of variables of interest given some evidence over some other variables (see image below).</p>
<div class="figure" style="text-align:center">
<img src="http://www.amidsttoolbox.com/wp-content/uploads/2018/04/staticinference.png" alt="Learning process scheme" width="377" /><p class="caption" style="text-align:center">Learning process scheme<span data-label="fig:lvmodels:static:inference:scheme"></span></p>
</div>
<p>The inference process is the same regardless of the way we have learnt our model: we simply have to obtain the BN learnt (stored as an object of the class <em>BayesianNetwork</em>), set the target variables and the evidence. As an example, let us consider the following code:</p>
<pre><code>public class StaticModelInference {

    public static void main(String[] args) throws IOException, ClassNotFoundException {

        BayesianNetwork bn  = BayesianNetworkLoader.loadFromFile(&quot;networks/simulated/exampleBN.bn&quot;);
        Variables variables = bn.getVariables();

        //Variabeles of interest
        Variable varTarget = variables.getVariableByName(&quot;LatentVar1&quot;);
        Variable varObserved = null;

        //we set the evidence
        Assignment assignment = new HashMapAssignment(2);
        varObserved = variables.getVariableByName(&quot;Income&quot;);
        assignment.setValue(varObserved,0.0);

        //we set the algorithm
        InferenceAlgorithm infer = new VMP(); //new HuginInference(); new ImportanceSampling();
        infer.setModel(bn);
        infer.setEvidence(assignment);

        //query
        infer.runInference();
        Distribution p = infer.getPosterior(varTarget);
        System.out.println(&quot;P(LatentVar1|Income=0.0) = &quot;+p);

        //Or some more refined queries
        System.out.println(&quot;P(0.7&lt;LatentVar1&lt;6.59 |Income=0.0) = &quot; + infer.getExpectedValue(varTarget, v -&gt; (0.7 &lt; v &amp;&amp; v &lt; 6.59) ? 1.0 : 0.0 ));

    }

}</code></pre>
<p><a href="https://github.com/amidst/tutorial/blob/master/src/main/java/eu/amidst/tutorial/usingAmidst/examples/StaticModelInference.java">See on GitHub</a></p>
<p>Note that the learning algorithm can be easily changed by simply modifying line 35 where <em>VMP algorithm</em>. If we aim to use <em>Importance Sampling algorithm</em>, replace such line with:</p>
<pre><code>    InferenceAlgorithm infer = new ImportanceSampling();</code></pre>
<p>Alternatively, we can use <em>Hugin Inference algorithm</em> (assuming that we have the corresponding libraries):</p>
<pre><code>InferenceAlgorithm infer = new HuginInference();</code></pre>
<h3 id="sec:lvmodels:static:custom">Custom static model</h3>
<p>It could happend that your model of interest is not predifined. In that case you can implement it yourself. For that purpose, create a new class inheriting the class <em>Model</em>. Then, add the code to the constructor with an object <em>Attributes</em> as input parameter, and the code of the method <em>void buildDAG()</em>. This last method is called before learning process and creates the object representing the DAG. As an example, the code below shows how to create a custom Gaussian Mixture.</p>
<pre><code>public class CustomGaussianMixture extends Model{

    Attributes attributes;

    public CustomGaussianMixture(Attributes attributes) throws WrongConfigurationException {
        super(attributes);
        this.attributes=attributes;
    }

    @Override
    protected void buildDAG() {

        /** Create a set of variables from the given attributes**/
        Variables variables = new Variables(attributes);

        /** Create a hidden variable with two hidden states*/
        Variable hiddenVar = variables.newMultinomialVariable(&quot;HiddenVar&quot;,2);

        //We create a standard naive Bayes
        DAG dag = new DAG(variables);

        for (Variable variable: variables){
            if (variable==hiddenVar)
                continue;

            dag.getParentSet(variable).addParent(hiddenVar);
        }

        //This is needed to maintain coherence in the Model class.
        this.dag=dag;
        this.vars = variables;

    }

    //Method for testing the custom model
    public static void main(String[] args) {
        String filename = &quot;datasets/simulated/cajamar.arff&quot;;
        DataStream&lt;DataInstance&gt; data = DataStreamLoader.open(filename);

        //Learn the model
        Model model = new CustomGaussianMixture(data.getAttributes());

        model.updateModel(data);
        BayesianNetwork bn = model.getModel();

        System.out.println(bn);
    }


}</code></pre>
<p><a href="https://github.com/amidst/tutorial/blob/master/src/main/java/eu/amidst/tutorial/usingAmidst/practice/CustomGaussianMixture.java">See on GitHub</a></p>
<h2 id="sec:lvmodels:dynamic">Dynamic Models</h2>
<h3 id="sec:lvmodels:dynamic:learning"> Learning and saving to disk </h3>
<p>When dealing with temporal data, it might be advisable to learn a dynamic model. The module <em>latent-variable-models</em> in AMDIST also suports such kinds of models. For that the classes inheriting <em>DynamicModel</em> will be used. A synthesis of the learning process is shown below.</p>
<div class="figure" style="text-align:center">
<img src="http://www.amidsttoolbox.com/wp-content/uploads/2018/04/dynamicLearning.png" alt="Learning process scheme of a dynamic model" width="377" /><p class="caption" style="text-align:center">Learning process scheme of a dynamic model<span data-label="fig:lvmodels:dynamic:learning:scheme"></span></p>
</div>
<p>A code-example is given below. Here, a <em>Hidden Markov Model (HMM)</em> is learnt from a financial dataset with temporal information. Note the difference within the static learning is the way the dataset is loaded: now, it is handled with an object of the class <em>DataStream&lt;DynamicDataInstance&gt;</em>. Finally, the DBN learnt is saved to disk with the method <em>DynamicBayesianNetworkWriter::save(String pathFile)</em>.</p>
<pre><code>public class DynamicModelLearning {

    public static void main(String[] args) throws IOException, ExceptionHugin {

        //Load the datastream
        String filename = &quot;datasets/simulated/cajamar.arff&quot;;
        DataStream&lt;DynamicDataInstance&gt; data = DynamicDataStreamLoader.loadFromFile(filename);


        //Learn the model
        DynamicModel model = new HiddenMarkovModel(data.getAttributes());
        model.updateModel(data);
        DynamicBayesianNetwork dbn = model.getModel();


        System.out.println(dbn);


        // Save with .bn format
        DynamicBayesianNetworkWriter.save(dbn, &quot;networks/simulated/exampleDBN.dbn&quot;);

        // Save with hugin format
        //DynamicBayesianNetworkWriterToHugin.save(dbn, &quot;networks/simulated/exampleDBN.net&quot;);

    }


}</code></pre>
<p><a href="https://github.com/amidst/tutorial/blob/master/src/main/java/eu/amidst/tutorial/usingAmidst/examples/DynamicModelLearning.java">See on GitHub</a></p>
<h3 id="inference">Inference</h3>
<p>In the following code-example, the inference process of dynamic models is illustrated. First, a DBN is loaded from disk (line 24). Then, a dynamic dataset is loaded for testing our model (lines 29 to 30). Then the inference algorithm and target variables are set. In the final loop, the inference is perform for each data instance.</p>
<pre><code>public class DynamicModelInference {

    public static void main(String[] args) throws IOException, ClassNotFoundException {

        DynamicBayesianNetwork dbn = DynamicBayesianNetworkLoader.loadFromFile(&quot;networks/simulated/exampleDBN.dbn&quot;);

        System.out.println(dbn);

        //Testing dataset
        String filenamePredict = &quot;datasets/simulated/cajamar.arff&quot;;
        DataStream&lt;DynamicDataInstance&gt; dataPredict = DynamicDataStreamLoader.open(filenamePredict);

        //Select the inference algorithm
        InferenceAlgorithmForDBN infer = new FactoredFrontierForDBN(new ImportanceSampling()); // new ImportanceSampling(),  new VMP(),
        infer.setModel(dbn);


        Variable varTarget = dbn.getDynamicVariables().getVariableByName(&quot;discreteHiddenVar&quot;);
        UnivariateDistribution posterior = null;

        //Classify each instance
        int t = 0;
        for (DynamicDataInstance instance : dataPredict) {
            if (instance.getSequenceID()&gt;0)
                break;

            infer.addDynamicEvidence(instance);
            infer.runInference();

            posterior = infer.getFilteredPosterior(varTarget);
            System.out.println(&quot;t=&quot;+t+&quot;, P(discreteHiddenVar | Evidence)  = &quot; + posterior);

            posterior = infer.getPredictivePosterior(varTarget, 2);
            //Display the output
            System.out.println(&quot;t=&quot;+t+&quot;+5, P(discreteHiddenVar | Evidence)  = &quot; + posterior);


            t++;
        }





    }


}</code></pre>
<p><a href="https://github.com/amidst/tutorial/blob/master/src/main/java/eu/amidst/tutorial/usingAmidst/examples/DynamicModelInference.java">See on GitHub</a></p>
<p>Note that the inference algorithm can be easily change if line 33 is modified by replacing it with:</p>
<pre><code>InferenceAlgorithmForDBN = new FactoredFrontierForDBN(new VMP());</code></pre>
<h3 id="sec:lvmodels:dynamic:custom">Custom dynamic model</h3>
<p>Like for static models, we might be interested in creating our own dynamic models. In this case, you will have to create a class inheriting <em>DynamicModel</em>. Here below an example of a custom <em>Kalman Filter</em> is given.</p>
<pre><code>public class CustomKalmanFilter extends DynamicModel {

    Attributes attributes;

    public CustomKalmanFilter(Attributes attributes) throws WrongConfigurationException {
        super(attributes);
        this.attributes=attributes;
    }


    @Override
    protected void buildDAG() {

        /*number of continuous hidden variable*/
        int numHiddenVars=3;

        /** Create a set of dynamic variables from the given attributes**/
        variables = new DynamicVariables(attributes);

        /* List of continuous hidden vars*/
        List&lt;Variable&gt; gaussianHiddenVars = new ArrayList&lt;&gt;();

        for(int i=0; i&lt;numHiddenVars; i++) {
            Variable Hi = variables.newGaussianDynamicVariable(&quot;gaussianHiddenVar&quot; + i);
            gaussianHiddenVars.add(Hi);
        }

        DynamicDAG dynamicDAG = new DynamicDAG(this.variables);


        for (Variable h : gaussianHiddenVars) {
            dynamicDAG.getParentSetTimeT(h).addParent(h.getInterfaceVariable());
        }

        for (Variable variable: variables) {
            if (gaussianHiddenVars.contains(variable))
                continue;

            for (Variable h : gaussianHiddenVars) {
                dynamicDAG.getParentSetTimeT(variable).addParent(h);
            }
        }

        //This is needed to maintain coherence in the DynamicModel class.
        this.variables = variables;
        this.dynamicDAG = dynamicDAG;
    }


    public static void main(String[] args) throws IOException, ExceptionHugin {

        //Load the datastream
        String filename = &quot;datasets/simulated/cajamar.arff&quot;;
        DataStream&lt;DynamicDataInstance&gt; data = DynamicDataStreamLoader.loadFromFile(filename);


        //Learn the model
        DynamicModel model = new CustomKalmanFilter(data.getAttributes());
        model.updateModel(data);
        DynamicBayesianNetwork dbn = model.getModel();


        System.out.println(dbn);

    }


}</code></pre>
<p><a href="https://github.com/amidst/tutorial/blob/master/src/main/java/eu/amidst/tutorial/usingAmidst/practice/CustomKalmanFilter.java">See on GitHub</a></p>

